# Imports
import cv2  # pip install opencv-python es el m칩dulo de open cv
import numpy as np  #
import matplotlib.pyplot as plt
# %matplotlib inline # en cuadernos jupiter para poder mostrar directamente las im치genes del cuaderno
from IPython.display import Image  # nos permitir치 mostrar y renderizar im치genes directamente en el cuaderno.

# *****************************************************************************************************************
# ***** Leer, imprimir, split, mergear, Conversi칩n a diferentes espacios de color y guardar im치genes usando OpenCV
# *****************************************************************************************************************
Image(filename='checkerboard_18x18.png')  # mostrar 18x18 pixel image (en notebook) solo .

# ***** Leer e imprimir im치genes usando OpenCV
'''
OpenCV permite leer diferentes tipos de im치genes (JPG, PNG, etc). Puede cargar im치genes en escala de grises, im치genes 
en color o tambi칠n puede cargar im치genes con canal alfa. Utiliza la funci칩n cv2.imread() que tiene la siguiente 
sintaxis:
- retval = cv2.imread( nombre de archivo[, banderas] )
- retval: Es la imagen si se carga correctamente. De lo contrario, es None. Esto puede suceder si el nombre del 
  archivo es incorrecto o si el archivo est치 da침ado.

La funci칩n tiene 1 argumento de entrada obligatorio y un indicador opcional:

- nombre de archivo: puede ser una ruta absoluta o relativa. Este es un argumento obligatorio.
- Flags: estas banderas se utilizan para leer una imagen en un formato particular (por ejemplo, 
            escala de grises/color/con canal alfa). Este es un argumento opcional con un valor predeterminado de cv2.
            IMREAD_COLOR o 1 que carga la imagen como una imagen en color.
Flags disponibles:

- cv2.IMREAD_GRAYSCALE o 0: Carga la imagen en modo escala de grises
- cv2.IMREAD_COLOR o 1: Carga una imagen a color. Se descuidar치 cualquier transparencia de la imagen. Es la bandera 
  por defecto.
- cv2.IMREAD_UNCHANGED o -1: Carga la imagen como tal, incluido el canal alfa.

Documentaci칩n OpenCV
**Imread:**https://docs.opencv.org/4.5.1/d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56
ImreadModes: https://docs.opencv.org/4.5.1/d8/d6a/group__imgcodecs__flags.html#ga61d9b0126a3e57d9277ac48327799c80
'''

# leer la imagen en escala de grises e introducirlo en la variable img
img = cv2.imread('checkerboard_18x18.png', 0)  # cargamos la imagen con imread, o en escala de grises
# Lo que se carga en mi memoria es una matriz 2D de Numpy que representa la imagen.

print(img)  # pintarlo en consola
'''
[[  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]]

Son 18 filas y 18 columnas, y cada uno de los valores representa las intensidades de p칤xel para cada uno de esos p칤xeles
Y observe que est치n en el rango de 0-255 porque esta imagen est치 siendo representada por un entero  de8-bit unsigned 
integer (0 a 255).
'''

# Imprime el tama침o de la imagen
print("Image size is ", img.shape)  # Image size is  (18, 18)

# Imprime el tipo de dato de la imagen
print("Data type of image is ", img.dtype)  # Data type of image is  uint8

# **** mostrar imagen en una ventana
plt.imshow(img)
plt.show()
''' lo muestra mediante a lib de matploit en notebook como una represantaci칩n de puntos corrdenadas x/y
es en realidad un gr치fico o una representaci칩n matem치tica de esa imagen, pero no es 18 p칤xeles de ancho en mi pantalla,
es solo una trama que representa 18 p칤xeles. Y la raz칩n de esto es que map plot lib usa mapas de color para representar
datos de imagen.'''
plt.imshow(img, cmap='gray')  # con el flag opcional seleccionamos formato en escala de grises


# ++++ ejemplo imagen escala de grises +++++
cb_img_fuzzy = cv2.imread("checkerboard_fuzzy_18x18.jpg", 0)

print(cb_img_fuzzy)  # pintarlo en consola
'''[[  0   0  15  20   1 134 233 253 253 253 255 229 130   1  29   2   0   0]
 [  0   1   5  18   0 137 232 255 254 247 255 228 129   0  24   2   0   0]
 [  7   5   2  28   2 139 230 254 255 249 255 226 128   0  27   3   2   2]
 [ 25  27  28  38   0 129 236 255 253 249 251 227 129   0  36  27  27  27]
 [  2   0   0   4   2 130 239 254 254 254 255 230 126   0   4   2   0   0]
 [132 129 131 124 121 163 211 226 227 225 226 203 164 125 125 129 131 131]
 [234 227 230 229 232 205 151 115 125 124 117 156 205 232 229 225 228 228]
 [254 255 255 251 255 222 102   1   0   0   0 120 225 255 254 255 255 255]
 [254 255 254 255 253 225 104   0  50  46   0 120 233 254 247 253 251 253]
 [252 250 250 253 254 223 105   2  45  50   0 127 223 255 251 255 251 253]
 [254 255 255 252 255 226 104   0   1   1   0 120 229 255 255 254 255 255]
 [233 235 231 233 234 207 142 106 108 102 108 146 207 235 237 232 231 231]
 [132 132 131 132 130 175 207 223 224 224 224 210 165 134 130 136 134 134]
 [  1   1   3   0   0 129 238 255 254 252 255 233 126   0   0   0   0   0]
 [ 20  19  30  40   5 130 236 253 252 249 255 224 129   0  39  23  21  21]
 [ 12   6   7  27   0 131 234 255 254 250 254 230 123   1  28   5  10  10]
 [  0   0   9  22   1 133 233 255 253 253 254 230 129   1  26   2   0   0]
 [  0   0   9  22   1 132 233 255 253 253 254 230 129   1  26   2   0   0]]
'''
# mostrar imagen.
plt.imshow(cb_img_fuzzy, cmap='gray')
plt.show()


# ++++ ejemplo imagen Cocacola de grises +++++
# Read and display Coca-Cola logo.
Image("coca-cola-logo.png")  # mostrar el logo de coca-cola (en notebook) solo .
coke_img = cv2.imread("coca-cola-logo.png", 1)  # leer imagen. opci칩n formato color
print("Image size is ", coke_img.shape)  # Tama침o de la imagen, Image size is  (700, 700, 3) se ve que tiene 3 canales
print("Data type of image is ", coke_img.dtype)   # Mostrar el tipo de dato de la imagen, Data type of image is  uint8
plt.imshow(coke_img)
plt.show()
'''
El color que se muestra por defecto es diferente de la imagen real. Esto se debe a que 
matplotlib espera la imagen en formato RGB mientras que OpenCV almacena im치genes en formato BGR. Por lo tanto, para una 
visualizaci칩n correcta, necesitamos invertir los canales de la imagen

Lo de [::-1] es un "truco" frecuentemente usado en python para obtener una lista o una cadena "del rev칠s". 
Se basa en el operador slice (rodaja) cuya sintaxis general es:
- iterable[inicio:fin:paso]
que permite extraer una serie de elementos del iterable, comenzando por el numerado como inicio y terminando por el 
numerado como fin-1, aumentando de paso en paso.

Si omites inicio se empezar치 en el primer elemento del iterable, si omites fin se terminar치 en el 칰ltimo elemento del 
iterable. Si el paso es negativo, el iterable se recorre "hacia atr치s", y en ese caso los valores por defecto cuando se 
omite inicio y fin se invierten.

As칤 pues iterable[::-1] devuelve los elementos del iterable, comenzando por el 칰ltimo y terminando por el primero, 
en orden inverso a como estaban.'''

# No se va a ver bien a menos que cambiemos el orden del canal.
coke_img_channels_reversed = coke_img[:, :, ::-1]  # Invierte el orden de ese 칰ltimo miembro de la matriz (700, 700, 3)
plt.imshow(coke_img_channels_reversed)
plt.show()


# ***** Split y mege im치genes usando OpenCV
'''Divisi칩n y fusi칩n de canales de color
- cv2.split() Divide una matriz multicanal en varias matrices de un solo canal.
- cv2.merge() Combina varias matrices para crear una 칰nica matriz multicanal. Todas las matrices de entrada deben tener 
              el mismo tama침o.
Documentaci칩n de OpenCV

https://docs.opencv.org/4.5.1/d2/de8/group__core__array.html#ga0547c7fed86152d7e9d0096029c8518a

A continuaci칩n vamos a cargar una imagen y posteriormente voy a llamar a la funci칩n de divisi칩n abierta para tomar esa 
imagen multicanal y dividirla en sus componentes. B, G y R. Y as칤, cada una de estas variables representa una matriz 
numpy 2D que contiene las intensidades de p칤xeles para esos canales de colores.
'''
# Split de la imagen en los componentes B, G, R
img_NZ_bgr = cv2.imread("New_Zealand_Lake.jpg", cv2.IMREAD_COLOR)
b, g, r = cv2.split(img_NZ_bgr)  # slpit + desempaquetado

# Ahora, simplemente usaremos I am show para mostrar cada uno de esas representaciones como un mapa en escala de grises
plt.figure(figsize=[20, 5])
plt.subplot(141); plt.imshow(r, cmap='gray');plt.title("Red Channel");
plt.subplot(142); plt.imshow(g, cmap='gray');plt.title("Green Channel");
plt.subplot(143); plt.imshow(b, cmap='gray');plt.title("Blue Channel");
'''
Y luego este 칰ltimo fragmento de c칩digo toma esos canales individuales y usa la funci칩n de fusi칩n para fusionar ellos de
nuevo en lo que deber칤a ser la imagen original. Y llamaremos a esa imagen fusionada aqu칤, y tambi칠n la mostraremos.
Y vale la pena mencionar un poco aqu칤 que puede obtener algo de intuici칩n con solo echar un vistazo en la imagen 
original. 
por ejemplo, este lago es una especie de azul turquesa, por as칤 decirlo. Seguro que tiene algo de verde y azul, y 
probablemente muy poco de rojo. Entonces, si ahora regresa a estos canales, puede ver que el Canal Rojo para la parte 
del lago es bajo, lo que significa que no hay mucho componente rojo en ese color. Por eso es m치s oscuro. Est치 m치s cerca
de cero. Y f칤jate en el verde. Los canales azules tienen una intensidad bastante alta para sus respectivos colores, 
lo que indica que el color de esa agua tiene un poco de rojo, pero un poco de verde y definitivamente bastante azul.'''
# Merge de cada canal en una imagen BGR
imgMerged = cv2.merge((b, g, r))
# mostramos la imagen mergeada (Invertimos el orden de ese 칰ltimo miembro de la matriz)
plt.subplot(144); plt.imshow(imgMerged[:, :, ::-1]); plt.title("Merged Output");
plt.show()


# ***** Conversi칩n a diferentes espacios de color
'''
- cv2.cvtColor() Convierte una imagen de un espacio de color a otro. La funci칩n convierte una imagen de entrada de un 
                espacio de color a otro. En caso de una transformaci칩n del espacio de color RGB, el orden de los canales 
                debe especificarse expl칤citamente (RGB o BGR). Tenga en cuenta que el formato de color predeterminado 
                en OpenCV a menudo se denomina RGB, pero en realidad es BGR (los bytes est치n invertidos). Entonces, 
                el primer byte en una imagen de color est치ndar (24 bits) ser치 un componente azul de 8 bits, el segundo 
                byte ser치 verde y el tercer byte ser치 rojo. Los bytes cuarto, quinto y sexto ser칤an entonces el segundo 
                p칤xel (azul, luego verde, luego rojo), y as칤 sucesivamente.

Sintaxis de la funci칩n
dst = cv2.cvtColor(origen, c칩digo)
dst: es la imagen de salida del mismo tama침o y profundidad que src.

La funci칩n tiene 2 argumentos requeridos:
- imagen de entrada src: 8 bits sin firmar, 16 bits sin firmar ( CV_16UC... ) o punto flotante de precisi칩n simple.
- c칩digo: c칩digo de conversi칩n de espacio de color (consulte ColorConversionCodes).
Documentaci칩n OpenCV
cv2.cvtColor: https://docs.opencv.org/3.4/d8/d01/group__imgproc__color__conversions.html#ga397ae87e1288a81d2363b61574eb8cab 
ColorConversionCodes: https://docs.opencv.org/4.5.1/d8/d01/group__imgproc__color__conversions.html#ga4e0972be5de079fed4e3a10e24ef5ef0'''

# Cambiando BGR a RGB
# OpenCV almacena los canales de color en un orden diferente al de la mayor칤a de las otras aplicaciones (BGR vs RGB).
img_NZ_rgb = cv2.cvtColor(img_NZ_bgr, cv2.COLOR_BGR2RGB)  # estamos pasando la imagen y un flag que indica la conversi칩n
plt.imshow(img_NZ_rgb)  # con el cambio Simplemente estamos mostrando la imagen original.
plt.show()

'''
vamos a convertir la representaci칩n BGR de esa imagen en un HSV representaci칩n. HSV significa saturaci칩n y valor de tono
, y ese es otro espacio de color que se usa a menudo en la imagen. Procesamiento y visi칩n por computadora.

Y entonces vamos a almacenar ese resultado en una variable llamada image subrayado HSV. As칤 que ahora puedo dividir esos
canales como hicimos anteriormente y obtener los componentes HSN V, por ejemplo.

H representa el color de la saturaci칩n de la imagen, S representa la intensidad del color y V representa el valor, es 
decir, puede pensar en la saturaci칩n como un rojo puro versus un rojo opaco, y puede pensar en el valor S ( intensidad)
como cu치n blanco u oscuro es el color, independientemente del color en s칤. Y luego Hugh se parece m치s a la 
representaci칩n del color real.
'''
img_hsv = cv2.cvtColor(img_NZ_bgr, cv2.COLOR_BGR2HSV)  # conversi칩n
h, s, v = cv2.split(img_hsv)  # Split de la imagen al desempaquetado de los componentes h, s, v


# mostramos los canales
plt.figure(figsize=[20, 5])
plt.subplot(141);plt.imshow(h,cmap='gray');plt.title("H Channel");
plt.subplot(142);plt.imshow(s,cmap='gray');plt.title("S Channel");
plt.subplot(143);plt.imshow(v,cmap='gray');plt.title("V Channel");
plt.subplot(144);plt.imshow(img_NZ_rgb);plt.title("Original");
plt.show()

'''
vamos a modificar uno de los canales. Si observa esta primera l칤nea de c칩digo, tomaremos el valor Q actual y le 
sumaremos 10 a eso. As칤 que solo estamos cambiando donde estamos en el espectro de color y luego fusionar칠 ese nuevo 
canal con los canales de arena originales, obteniendo una imagen fusionada, y luego usaremos el cvtColor() para 
convertir eso de HSV a GB.

En definitiva modifiqu칠 uno de los canales, lo fusion칠 y ahora lo convert칤.

por ello podremos ver la imagen modificada, porque hemos cambiado el tono, observandose diferente a la imagen original 
'''
# Cambio de la saturaci칩n
h_new = h+10
img_NZ_merged = cv2.merge((h_new, s, v))
img_NZ_rgb = cv2.cvtColor(img_NZ_merged, cv2.COLOR_HSV2RGB)

# mostramos los canales y la imagen
plt.figure(figsize=[20,5])
plt.subplot(141);plt.imshow(h,cmap='gray');plt.title("H Channel");
plt.subplot(142);plt.imshow(s,cmap='gray');plt.title("S Channel");
plt.subplot(143);plt.imshow(v,cmap='gray');plt.title("V Channel");
plt.subplot(144);plt.imshow(img_NZ_rgb);plt.title("Modified");
plt.show()


# ***** Guardar imagen cv2.imwrite()
'''
Guardar la imagen es tan trivial como leer una imagen en OpenCV. Usamos la funci칩n cv2.imwrite() con dos argumentos. El 
primero es el nombre del archivo, el segundo argumento es el objeto de la imagen.

La funci칩n imwrite guarda la imagen en el archivo especificado. El formato de imagen se elige en funci칩n de la 
extensi칩n del nombre de archivo (consulte cv::imread para ver la lista de extensiones). En general, solo las im치genes 
de 8 bits de un solo canal o de 3 canales (con orden de canales 'BGR') se pueden guardar con esta funci칩n.

Sintaxis de la funci칩n
cv2.imwrite (nombre de archivo, img [, par치metros])
La funci칩n tiene 2 argumentos requeridos:

- nombre de archivo: puede ser una ruta absoluta o relativa.
- img: Imagen o Im치genes a guardar.
Documentaci칩n OpenCV
Imwrite: https://docs.opencv.org/4.5.1/d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce 
**ImwriteFlags:**https://docs.opencv.org/4.5.1/d8/d6a/group__imgcodecs__flags.html#ga292d81be8d76901bff7988d18d2b42ac
'''
# Guardar la imagen
cv2.imwrite("New_Zealand_Lake_SAVED.png", img_NZ_bgr)

# leemos la imagen guardada
# read the image as Color
img_NZ_bgr = cv2.imread("New_Zealand_Lake_SAVED.png", cv2.IMREAD_COLOR)
print("img_NZ_bgr shape is: ", img_NZ_bgr.shape)  # img_NZ_bgr shape is:  (600, 840, 3)

# read the image as Grayscaled
img_NZ_gry = cv2.imread("New_Zealand_Lake_SAVED.png", cv2.IMREAD_GRAYSCALE)
print("img_NZ_gry shape is: ", img_NZ_gry.shape)  # img_NZ_gry shape is:  (600, 840)


# ***** mostrar la imagen con matploit o con opencv

window1 = cv2.namedWindow("w1")  # creamos una ventana
cv2.imshow('image', img_NZ_bgr, )  # llamamos al show de OpenCV, OJO como es el de Open cv se guarda y muestra en BGR
cv2.waitKey(0)  # pulsar una tecla para cerrar la imagen OpenCV si 0, si ponemos numeros seran los segundos de espera

# cv2.waitKey(8000)   # 8 segundos

# keypress = cv2.waitKey(0)  # creamos una variable que contenga la primera tecla introducida
# if keypress == ord('q'):   # si la tecla ( es en ascii) coincide con el ascii de q
#     Alive = False

cv2.destroyWindow(window1)  # destruimos la ventana creada

# *************************************
# ***** Manipulaci칩n b치sica de im치genes
# *************************************

# Cargamos imagen original de pruebas en escala de grises
cb_img = cv2.imread("checkerboard_18x18.png", 0)

# Establezca el mapa de colores en escala de grises para una representaci칩n adecuada.
plt.imshow(cb_img, cmap='gray')
plt.show()
print(cb_img)
'''
[[  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]]'''

# ****** Acceso a p칤xeles individuales
'''
Veamos c칩mo acceder a un p칤xel en la imagen. Para acceder a cualquier p칤xel en una matriz numpy, debe usar notaci칩n de 
matriz:
- matrix[r,c], donde r es el n칰mero de fila y c es el n칰mero de columna. la matriz est치 indexada en 0.
Por ejemplo, si desea acceder al primer p칤xel, debe especificar matrix[0,0]. Veamos con algunos ejemplos. 
Imprimiremos un p칤xel negro desde la parte superior izquierda y un p칤xel blanco desde la parte superior central.'''

# Imprime el primer pixel del primer cuadro negro
print(cb_img[0, 0])  # 0
# imprima el primer p칤xel blanco a la derecha del primer cuadro negro
print(cb_img[0, 6])  # 255

# ****** Modificando los p칤xeles de las im치genes

cb_img_copy = cb_img.copy()
# cb_img_copy[2, 2] = 200
# cb_img_copy[2, 3] = 200
# cb_img_copy[3, 2] = 200
# cb_img_copy[3, 3] = 200
cb_img_copy[2:4, 2:4] = 200  # Lo mismo que lo de antes de una

plt.imshow(cb_img_copy, cmap='gray')
plt.show()
print(cb_img_copy)

'''[[  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0 200 200   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0 200 200   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [255 255 255 255 255 255   0   0   0   0   0   0 255 255 255 255 255 255]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]
 [  0   0   0   0   0   0 255 255 255 255 255 255   0   0   0   0   0   0]]
'''

# ***** Recortar im치genes
''' Recortar una imagen se logra simplemente seleccionando una regi칩n espec칤fica (p칤xel) de la imagen.
Es simplemente indexar una imagen existente y extraer la regi칩n que le interesa.'''
img_NZ_bgr = cv2.imread("New_Zealand_Boat.jpg", cv2.IMREAD_COLOR)  # cargamos una imagen a color
img_NZ_rgb = img_NZ_bgr[:, :, ::-1]  # Invertimos el 칰ltimo color
plt.imshow(img_NZ_rgb)  # Mostramos la imagen
plt.show()  # para que se muestre

# Recortar la regi칩n media de la imagen
cropped_region = img_NZ_rgb[200:400, 300:600]
plt.imshow(cropped_region)
plt.show()

# ***** Cambiar el tama침o de las im치genes
'''La funci칩n de cambio de tama침o cambia el tama침o de la imagen src hacia abajo o hacia arriba hasta el tama침o 
especificado. El tama침o y el tipo se derivan de src,dsize,fx y fy.
sintaxis:

 - dst = resize( src, dsize[, dst[, fx[, fy[, interpolation]]]] )

El primero es la imagen de origen, y el segundo argumento requerido es el tama침o de salida deseado de la imagen.
Y luego hay varios argumentos opcionales  Fx e Fy,  los cu치les son sus factores de escala, que vamos a demostrar a 
continuaci칩n. luego est치 el m칠todo de interpolaci칩n ( hay varios a seleccionar ), Por ejemplo, cuando aumenta el tama침o 
de una imagen, tiene que inventar nuevos p칤xeles y, por lo tanto, hay una interpolaci칩n que se requiere para hacer eso. 

La funci칩n tiene 2 argumentos requeridos:

- src: imagen de entrada
- dsize: tama침o de la imagen de salida
Los argumentos opcionales que se utilizan a menudo incluyen:
- fx: Factor de escala a lo largo del eje horizontal; cuando es igual a 0, se calcula como (洧뚨洧뚲洧륋롘洧뚯洧뚩)洧뚨洧뚶洧뉧롘洧뚩.洧멇롘뉧롘꽳롘뢣롘/洧뚶洧뚵洧뚧.洧뚧洧뚲洧뚯洧뚶
- fy: Factor de escala a lo largo del eje vertical; cuando es igual a 0, se calcula como (洧뚨洧뚲洧륋롘洧뚯洧뚩)洧뚨洧뚶洧뉧롘洧뚩.洧뚬洧뚩洧뉧롘넗롘놿롘/洧뚶洧뚵洧뚧.洧뚵洧뚲洧멇롘

La imagen de salida tiene el tama침o dsize (cuando no es cero) o el tama침o calculado a partir de src.size(), fx y fy; el 
tipo de dst es el mismo que el de src.

Documentaci칩n OpenCV
 https://docs.opencv.org/4.5.0/da/d54/group__imgproc__transform.html#ga47a974309e9102f5f08231edc7e7529d
'''

# m칠todo 1: Especificaci칩n del factor de escala usando fx y fy
resized_cropped_region_2x = cv2.resize(cropped_region, None, fx=2, fy=2)
'''imagen, tama침o de salida ( al usar escala esta bien ponerlo a None factores de escala fx y tener Y.En este ejemplo, 
s칩lo vamos a establecerlos en dos As칤 que vamos a duplicar el tama침o de esta regi칩n recortada.Ahora tiene cuatrocientos 
p칤xeles de alto y 600 p칤xeles de ancho.'''
plt.imshow(resized_cropped_region_2x)
plt.show()

# m칠todo 2: Especificaci칩n del tama침o exacto de la imagen de salida
desired_width = 100
desired_height = 200
dim = (desired_width, desired_height)
'''
vamos a establecer un ancho y alto espec칤ficos para la imagen. en este caso, cien y doscientos respectivamente , y vamos
a crear este vector bidimensional indicando ambas dimensiones y lo usamos como segundo argumento para la funci칩n de 
cambio de tama침o y mostramos la regi칩n recortada redimensionada. la imagen se ha distorsionado ahora porque no 
mantuvimos la relaci칩n de aspecto original.'''
# Cambiar el tama침o de la imagen de fondo al mismo tama침o que la imagen del logotipo
resized_cropped_region = cv2.resize(cropped_region, dsize=dim, interpolation=cv2.INTER_AREA)
plt.imshow(resized_cropped_region)
plt.show()

# Cambiar el tama침o manteniendo la relaci칩n de aspecto
# m칠todo 2: usando 'dsize'
desired_width = 100
''' ahora vamos a comenzar especificando un ancho de 100 y luego calcularemos la altura deseada asociada manteniendo 
la relaci칩n de aspecto. As칤 que aqu칤 estamos creando esta proporci칩n del ancho deseado al ancho original de la imagen 
y luego usando ese factor para derivar la altura deseada aqu칤. cuando pasamos esa dimensi칩n revisada a la funci칩n de 
cambio de tama침o, obtenemos una imagen de cien p칤xeles de ancho y la cantidad adecuada de alto para mantener la 
relaci칩n adecuada, que resulta ser de unos sesenta y siete p칤xeles.

------------------------------------------------------------
para saber el ancho y el alto  funci칩n shape() (dimensiones) 
------------------------------------------------------------
dimensiones de una imagen dada, como la altura de la imagen, el ancho de la imagen y la cantidad de canales en la 
imagen, se denominan shape (forma) de la imagen y  se almacena en numpy.ndarray.
La funci칩n shape() puede proporcionar la dimensi칩n de una imagen dada y almacena cada una de las dimensiones de la 
imagen, como la altura de la imagen, el ancho de la imagen y la cantidad de canales en la imagen en diferentes 칤ndices.

La altura de la imagen se almacena en el 칤ndice 0.
El ancho de la imagen se almacena en el 칤ndice 1.
El n칰mero de canales en la imagen se almacena en el 칤ndice 2.

Ejemplo:

dimensions = input_image.shape
height = input_image.shape[0] 
width = input_image.shape[1] 
number_of_channels = input_image.shape[2]

- input_image: representa la imagen cuyas dimensiones se van a encontrar.
- dimensions: representan las dimensiones de la imagen.
- height: representa la altura de la imagen de entrada.
- width: representa el ancho de la imagen de entrada.
- number_of_channels: representa el n칰mero de canales en la imagen.

La relaci칩n de aspecto o ratio de una imagen es la proporci칩n entre el ancho y la altura de la imagen. Se calcula
dividiendo la anchura entre la altura, y se expresa normalmente con dos n칰meros separados por dos puntos. Por ejemplo 
3:2, significa que por cada tres unidades a lo largo hay dos unidades a lo alto
'''
print(cropped_region.shape[1])
aspect_ratio = desired_width / cropped_region.shape[1]  # calculamos el radio de aspecto
desired_height = int(cropped_region.shape[0] * aspect_ratio)  # calculamos la nueva altura
dim = (desired_width, desired_height)

resized_cropped_region = cv2.resize(cropped_region, dsize=dim, interpolation=cv2.INTER_AREA)  # Cambiar el tama침o de img
plt.imshow(resized_cropped_region)
plt.imshow(resized_cropped_region)

# Ahora, salvemos la imagen redimensionada (recortada)
# cambiamos el orden del canal
resized_cropped_region_2x = resized_cropped_region_2x[:, :, ::-1]

# Save resized image to disk
cv2.imwrite("resized_cropped_region_2x.png", resized_cropped_region_2x)

# ****** Voltear im치genes췄
''' Puedes voltearlo horizontalmente, verticalmente o en ambas direcciones
La funci칩n flip voltea la matriz en una de tres formas diferentes (los 칤ndices de fila y columna est치n basados en 0):

Sintaxis de la funci칩n
dst = cv.flip( src, flipCode )
* dst: matriz de salida del mismo tama침o y tipo que src.

La funci칩n tiene 2 argumentos requeridos:
- src: imagen de entrada
- flipCode: un flag para especificar c칩mo voltear la matriz; 
    - 0 significa girar alrededor del eje x, o voltearlo verticalmente ( boca abajo)
    - un valor positivo (por ejemplo, 1) significa girar alrededor del eje y, o voltearlo horizontalmente  (espejo)
    - Un valor negativo (por ejemplo, -1) significa girar alrededor de ambos ejes.
Documentaci칩n OpenCV
flip: https://docs.opencv.org/4.5.0/d2/de8/group__core__array.html#gaca7be533e3dac7feb70fc60635adf441
'''

img_NZ_rgb_flipped_horz = cv2.flip(img_NZ_rgb, 1)
img_NZ_rgb_flipped_vert = cv2.flip(img_NZ_rgb, 0)
img_NZ_rgb_flipped_both = cv2.flip(img_NZ_rgb, -1)

# mostramos las im치genes
plt.figure(figsize=[18, 5])
plt.subplot(141);plt.imshow(img_NZ_rgb_flipped_horz);plt.title("Horizontal Flip");
plt.subplot(142);plt.imshow(img_NZ_rgb_flipped_vert);plt.title("Vertical Flip");
plt.subplot(143);plt.imshow(img_NZ_rgb_flipped_both);plt.title("Both Flipped");
plt.subplot(144);plt.imshow(img_NZ_rgb);plt.title("Original");
plt.show()

# ***************************
# ***** Anotaci칩n de im치genes
# ***************************

# Leemos la imagen
image = cv2.imread("Apollo_11_Launch.jpg", cv2.IMREAD_COLOR)

# Mostramos la imagen original
plt.imshow(image[:, :, ::-1])
plt.show()

# ******  Dibujar una l칤nea
'''
Comencemos dibujando una l칤nea en una imagen. Usaremos la funci칩n cv2.line para esto.
Sintaxis
img = cv2.line(img, pt1, pt2, color[, grosor[, lineType[, shift]]])
img: La imagen de salida que ha sido anotada.

La funci칩n tiene 4 argumentos requeridos:
- img: Imagen sobre la que dibujaremos una l칤nea
- pt1: primer punto (ubicaci칩n x, y) del segmento de l칤nea
- pt2: Segundo punto del segmento de recta
- color: Color de la l칤nea que se dibujar치
Otros argumentos opcionales que es importante que sepamos incluyen:
- grosor: Entero que especifica el grosor de la l칤nea. El valor predeterminado es 1.
- lineType: Tipo de l칤nea. El valor predeterminado es 8, que representa una l칤nea conectada a 8. Por lo general, se 
 cv2.LINE_AA (l칤nea suavizada o suavizada) para el tipo de l칤nea.

Documentaci칩n de OpenCV췄
https://docs.opencv.org/4.5.1/d6/d6e/group__imgproc__draw.html#ga7078a9fae8c7e7d13d24dac2520ae4a2
'''
imageLine = image.copy()  # COPIAR UNA IMAGEN

'''
# La l칤nea comienza en (200,100) y termina en (400,100)
# El color de la l칤nea es AMARILLO (Recordemos que OpenCV usa formato BGR)
# El grosor de la l칤nea es 5px
# El tipo de l칤nea es cv2.LINE_AA'''

cv2.line(imageLine, (200, 100), (400, 100), (0, 255, 255), thickness=5, lineType=cv2.LINE_AA);

# Mostramos la imagen
plt.imshow(imageLine[:, :, ::-1])
plt.show()

# ******  Dibujar un c칤rculo

'''
c칤rculo en una imagen. Usaremos la funci칩n cv2.circle para esto.
sintaxis funcional
img = cv2.circle(img, centro, radio, color[, grosor[, tipo de l칤nea[, desplazamiento]]])
img: La imagen de salida que ha sido anotada.

La funci칩n tiene 4 argumentos requeridos:
- img: Imagen sobre la que dibujaremos una l칤nea
- centro: Centro del c칤rculo
- radio: Radio del c칤rculo
- color: Color del c칤rculo que se dibujar치
A continuaci칩n, echemos un vistazo a los argumentos (opcionales) que vamos a utilizar bastante.
- grosor: Grosor del contorno del c칤rculo (si es positivo). Si se proporciona un valor negativo para este argumento, dar치 como resultado un c칤rculo lleno.
- lineType: Tipo del l칤mite del c칤rculo. Esto es exactamente lo mismo que el argumento lineType en cv2.line
Documentaci칩n de OpenCV췄
c칤rculo: https://docs.opencv.org/4.5.1/d6/d6e/group__imgproc__draw.html#gaf10604b069374903dbd0f0488cb43670

'''
# Dibujamos el circulo
imageCircle = image.copy()
cv2.circle(imageCircle, (900, 500), 100, (0, 0, 255), thickness=5, lineType=cv2.LINE_AA)

# Mostramos la imagen
plt.imshow(imageCircle[:, :, ::-1])
plt.show()

# ******  Dibujar un rect치ngulo
''''
Usaremos la funci칩n cv2.rectangle para dibujar un rect치ngulo en una imagen. 

sintaxis 
img = cv2.rectangle(img, pt1, pt2, color[, grosor[, lineType[, shift]]])
img: La imagen de salida que ha sido anotada.

La funci칩n tiene 4 argumentos requeridos:
- img: Imagen sobre la que se va a dibujar el rect치ngulo.
- pt1: V칠rtice del rect치ngulo. Usualmente usamos el v칠rtice superior izquierdo aqu칤.
- pt2: V칠rtice del rect치ngulo opuesto a pt1. Usualmente usamos el v칠rtice inferior derecho aqu칤.
- color: color del rect치ngulo
A continuaci칩n, echemos un vistazo a los argumentos (opcionales) que vamos a utilizar bastante.
- grosor: Grosor del contorno del c칤rculo (si es positivo). Si se proporciona un valor negativo para este argumento, dar치 como resultado un rect치ngulo relleno.
- lineType: Tipo del l칤mite del c칤rculo. Esto es exactamente lo mismo que el argumento lineType en cv2.line
Enlaces de documentaci칩n de OpenCV
**rect치ngulo:**https://docs.opencv.org/4.5.1/d6/d6e/group__imgproc__draw.html#ga07d2f74cadcf8e305e810ce8eed13bc9
'''
# Dibujamos un rect치ngulo
imageRectangle = image.copy()
# pt1, pt2, esquina izquierda del rect치ngulo en la esquina inferior derecha del rect치ngulo.
cv2.rectangle(imageRectangle, (500, 100), (700, 600), (255, 0, 255), thickness=5, lineType=cv2.LINE_8);
# Mostramos la imagen
plt.imshow(imageRectangle[:, :, ::-1])
plt.show()

# ****** Agregar texto
'''
Para escribir texto en una imagen usando la funci칩n cv2.putText.

sintaxis funcional
img = cv2.putText(img, text, org, fontFace, fontScale, color[, thick[, lineType[, bottomLeftOrigin]]])
img: La imagen de salida que ha sido anotada.

La funci칩n tiene 6 argumentos requeridos:
- img: Imagen sobre la que se ha de escribir el texto.
- text: Cadena de texto a escribir.
- org: esquina inferior izquierda de la cadena de texto en la imagen.
- fontFace: tipo de fuente
- fontScale: factor de escala de fuente que se multiplica por el tama침o base espec칤fico de la fuente.
- color: color de fuente
Otros argumentos opcionales que es importante que sepamos incluyen:
- grosor: n칰mero entero que especifica el grosor de l칤nea del texto. El valor predeterminado es 1.
- lineType: Tipo de l칤nea. El valor predeterminado es 8, que representa una l칤nea conectada a 8. Por lo general, se usa 
cv2.LINE_AA (l칤nea suavizada o suavizada) para el tipo de l칤nea.

Documentaci칩n OpenCV
**poner texto:**https://docs.opencv.org/4.5.1/d6/d6e/group__imgproc__draw.html#ga5126f47f883d730f633d74f07456c576'''

imageText = image.copy()
text = "Apollo 11 Saturn V Launch, July 16, 1969"
fontScale = 2.3
fontFace = cv2.FONT_HERSHEY_PLAIN
fontColor = (0, 255, 0)
fontThickness = 2

cv2.putText(imageText, text, (200, 700), fontFace, fontScale, fontColor, fontThickness, cv2.LINE_AA);

# Mostramos la imagen
plt.imshow(imageText[:, :, ::-1])
plt.show()


# ******************************************
# ***** Operaciones aritm칠ticas con im치genes
# ******************************************
'''
Las t칠cnicas de procesamiento de im치genes aprovechan las operaciones matem치ticas para lograr diferentes resultados. 
La mayor칤a de las veces llegamos a una versi칩n mejorada de la imagen usando algunas operaciones b치sicas. Echaremos un 
vistazo a algunas de las operaciones fundamentales que se usan a menudo en las canalizaciones de visi칩n por computadora.
En este cuaderno cubriremos operaciones aritm칠ticas como la suma y la multiplicaci칩n.
'''
img_bgr = cv2.imread("New_Zealand_Coast.jpg", cv2.IMREAD_COLOR)  # cargar imagen a color [[[188 183 174],[189....
img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)  # cambiar el color a RGB (cv2 por defecto BGR)

# ***** Adici칩n o Brillo
'''
La primera operaci칩n que analizamos es la simple adici칩n de im치genes. Esto da como resultado aumentar o disminuir el 
brillo de la imagen ya que eventualmente estamos aumentando o disminuyendo los valores de intensidad de cada p칤xel en 
la misma cantidad. Entonces, esto resultar치 en un aumento/disminuci칩n global del brillo.

primera instrucci칩n matrix = ...
- numpy.ones(): devuelve un array del tama침o y tipo indicados inicializando sus valores con unos
- crea una matriz del tama침o img_rgb.shape (con la dimensi칩n de la imagen (600, 840, 3) es decir una imagen con el 
  tama침o de la original), tipo entero grande y con todo valor 50, es decir se crea una imagen que si la imprimimos es 
  un gris [[[50 50 50],v[50...
Y ahora simplemente vamos a usar las funciones de abrir, sumar y restar para sumar y restar esa matriz de la imagen 
original, siendo todo lo que se requiere para generar una imagen m치s oscura que la original y una imagen que es mas 
clara que la original
'''
matrix = np.ones(img_rgb.shape, dtype = "uint8") * 50
img_rgb_brighter = cv2.add(img_rgb, matrix)  # se le suma a la imagen original la matriz [[[224 233 238], [226...
img_rgb_darker   = cv2.subtract(img_rgb, matrix)  # se le resta a la imagen original la matriz [[[124 133 138], [122...
plt.subplot(131); plt.imshow(img_rgb_darker);  plt.title("Darker");
plt.subplot(132); plt.imshow(img_rgb);         plt.title("Original");
plt.subplot(133); plt.imshow(img_rgb_brighter);plt.title("Brighter");
plt.show()

# **** Multiplicaci칩n o Contraste
'''
Al igual que la suma puede resultar en un cambio de brillo, la multiplicaci칩n se puede usar para mejorar el contraste 
de la imagen. El contraste es la diferencia en los valores de intensidad de los p칤xeles dentro de una imagen. 
Multiplicar los valores de intensidad con una constante puede hacer que la diferencia sea mayor o menor (si el factor de
multiplicaci칩n es < 1).
'''
matrix1 = np.ones(img_rgb.shape) * .8  # Crea una matriz del mismo tama침o inicializado todo a 0.8 [[[0.8 0.8 ...
matrix2 = np.ones(img_rgb.shape) * 1.2  # Crea una matriz del mismo tama침o inicializado todo a 1.2 [[[1.2 1.2 ...

# convertimos los puntos de la imagen a flotante y multiplicamos por la matriz, convirtiendo despu칠s a un array de uint
# 8-bit unsigned integer (0 a 255).
img_rgb_darker = np.uint8(cv2.multiply(np.float64(img_rgb), matrix1))    # [[[139 146 150 ...
img_rgb_brighter = np.uint8(cv2.multiply(np.float64(img_rgb), matrix2))  # [[[208 219 255 ....
# mostramos las imagenes
plt.figure(figsize=[18,5])
plt.subplot(131); plt.imshow(img_rgb_darker);  plt.title("Lower Contrast");
plt.subplot(132); plt.imshow(img_rgb);         plt.title("Original");
plt.subplot(133); plt.imshow(img_rgb_brighter);plt.title("Higher Contrast");
plt.show()

'''la imagen de alto contraste, hay un c칩digo de color extra침o al mostrarlo, Y la raz칩n de esto es porque cuando 
multiplicamos la imagen original por esta matriz, tiene un factor de uno punto dos en ella. Potencialmente obtenemos 
valores superiores a 255. Entonces,  la imagen original aqu칤, las nubes  probablemente estaban cerca de 255. Algunos de 
ellos, al menos. Y cuando multiplicamos por uno punto dos, pasamos a cincuenta y cinco.

Entonces, cuando intentamos convertir esos valores en un n칰mero de ocho bits sin signo en lugar de exceder 255,
simplemente pasan a un n칰mero peque침o. provocando estos valores de intensidad cercanos a cero y siendo el motivo del
problema.

numpy.clip(): La funci칩n se utiliza para recortar (limitar) los valores en una matriz.
Dado un intervalo, los valores fuera del intervalo se recortan a los bordes del intervalo. Por ejemplo, si se especifica
 un intervalo de [0, 1], los valores menores que 0 se convierten en 0 y los valores mayores que 1 se convierten en 1.

Para solucionarlo lo que podemos hacer es usar la funci칩n clip de numpy para recortar primero esos valores al
rango de cero a 255 antes de convertirlos un entero de 8 bits (0-255), provocando que esta parte de la imagen se sature
por completo, teniendo algunos valores 255 por lo que realmente no tienen informaci칩n .
'''
matrix1 = np.ones(img_rgb.shape) * .8
matrix2 = np.ones(img_rgb.shape) * 1.2

img_rgb_lower = np.uint8(cv2.multiply(np.float64(img_rgb), matrix1))
img_rgb_higher = np.uint8(np.clip(cv2.multiply(np.float64(img_rgb), matrix2), 0, 255))

# Show the images
plt.figure(figsize=[18,5])
plt.subplot(131); plt.imshow(img_rgb_lower);  plt.title("Lower Contrast");
plt.subplot(132); plt.imshow(img_rgb);         plt.title("Original");
plt.subplot(133); plt.imshow(img_rgb_higher);plt.title("Higher Contrast");
plt.show()


# ******************************************
# ***** Operaciones bit a bit con im치genes
# ******************************************
'''
Las t칠cnicas de procesamiento de im치genes aprovechan diferentes operaciones l칩gicas para lograr diferentes resultados. 
La mayor칤a de las veces llegamos a una versi칩n mejorada de la imagen usando algunas operaciones l칩gicas b치sicas como 
las operaciones AND y OR.

Sintaxis:
 cv2.bitwise_and(). Otros incluyen: cv2.bitwise_or(), cv2.bitwise_xor(), cv2.bitwise_not()

dst = cv2.bitwise_and( src1, src2[, dst[, m치scara]] )
- dst: matriz de salida que tiene el mismo tama침o y tipo que las matrices de entrada.

La funci칩n tiene 2 argumentos requeridos:
- src1: primera matriz de entrada o un escalar.
- src2: segunda matriz de entrada o un escalar.
Un argumento opcional importante es:
- m치scara: m치scara de operaci칩n opcional, matriz de un solo canal de 8 bits, que especifica los elementos de la matriz 
de salida que se cambiar치n, es decir, a que parte de estas dos im치genes se aplica la l칩gica de la operaci칩n.

Documentaci칩n OpenCV
https://docs.opencv.org/4.5.1/d0/d86/tutorial_py_image_arithmetics.html 
https://docs.opencv.org/4.5.0/d2/de8/group__core__array.html#ga60b4d04b251ba5eb1392c34425497e14
'''
# leemos dos imagenes un rect치ngulo y un circulo.
img_rec = cv2.imread("rectangle.jpg", cv2.IMREAD_GRAYSCALE)
img_cir = cv2.imread("circle.jpg", cv2.IMREAD_GRAYSCALE)

plt.figure(figsize=[20, 5])
plt.subplot(121);
plt.imshow(img_rec, cmap='gray')
plt.subplot(122);
plt.imshow(img_cir, cmap='gray')
plt.show()
print(img_rec.shape)  # (200, 499)

# **** Operaci칩n not
''' En el operador NOT, cuando una entrada es verdadera o 1, su salida es falso o  0, y viceversa. En OpenCV se realiza 
el mismo procedimiento, con la diferencia que en vez de 1 se emplea 255, como he dicho antes, para poder visualizar el 
resultado o salida en colores blanco y negro'''
result = cv2.bitwise_not(img_rec)
plt.imshow(result, cmap='gray')
plt.show()

# **** Operaci칩n and
'''
Estamos pasando la imagen del rect치ngulo en la imagen del c칤rculo.Y luego estamos indicando que la m치scara es ninguna.
As칤 que simplemente vamos a hacer una comparaci칩n bit a bit entre estas dos im치genes y el valor devuelto de esa 
comparaci칩n ser치 255 (blanco) si los p칤xeles correspondientes en ambas im치genes son blancos.

Entonces, en este caso, el resultado ser치 solo este lado izquierdo de este semic칤rculo, ya que ese es el 칰nico regi칩n en
ambas im치genes donde los p칤xeles son blancos.'''
result = cv2.bitwise_and(img_rec, img_cir, mask=None)
plt.imshow(result, cmap='gray')
plt.show()

# **** Operaci칩n or
'''Ahora el valor de retorno de la operaci칩n ser치 blanco si el p칤xel correspondiente de cualquier punto de la imagen es 
blanco ( 255). EN este ejemplo, obtenemos todo el lado izquierdo del rect치ngulo, que es blanco y luego el lado derecho
lado de la mano del c칤rculo.'''
result = cv2.bitwise_or(img_rec, img_cir, mask=None)
plt.imshow(result, cmap='gray')
plt.show()

# **** Operaci칩n xor
''' Solo devolver치 un valor de blanco si el p칤xel correspondiente es blanco (255) en una imagen, pero no en ambas.'''
result = cv2.bitwise_xor(img_rec, img_cir, mask=None)
plt.imshow(result, cmap='gray')
plt.show()

# ******* Aplicaci칩n: manipulaci칩n de logotipos  ##########

# **** Leer imagen en primer plano
img_bgr = cv2.imread("coca-cola-logo.png")
img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
plt.imshow(img_rgb)
plt.show()
print(img_rgb.shape)
logo_w = img_rgb.shape[0]  # guardamos el ancho de la imagen
logo_h = img_rgb.shape[1]  # guardamos el alto de la imagen

# **** leer la imagen de fondo
# Leer en la imagen del fondo del tablero de color
img_background_bgr = cv2.imread("checkerboard_color.png")
img_background_rgb = cv2.cvtColor(img_background_bgr, cv2.COLOR_BGR2RGB)

# Establecer el ancho deseado (logo_w) y mantener la relaci칩n de aspecto de la imagen
aspect_ratio = logo_w / img_background_rgb.shape[1]
dim = (logo_w, int(img_background_rgb.shape[0] * aspect_ratio))

# Cambiar el tama침o de la imagen de fondo al mismo tama침o que la imagen del logotipo
img_background_rgb = cv2.resize(img_background_rgb, dim, interpolation=cv2.INTER_AREA)
plt.imshow(img_background_rgb)
plt.show()
print(img_background_rgb.shape)

# **** se cra una m치scara de la imagen de primer plano
img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)
'''vamos a pasar el logotipo aqu칤 para ver el color, convertirlo a gris. y luego use la treshold para 
crear una m치scara binaria a partir de la imagen en escala de grises.Entonces esto solo va a contener valores de cero y 
255.

Umbralizaci칩n o thresholding: Consiste en modificar una imagen a una representaci칩n binaria, por medio de la 
modificaci칩n de los valores de los pixeles estableciendo un valor umbral.

sintaxis: 
ret,thresh = cv2.threshold(img, umbral, valorMax , tipo)

Los par치metros son los siguientes:
- img es la imagen gris que va a ser analizada
- umbral es el valor indicado a analizar en cada p칤xel
- valorMax Valor que se coloca a un p칤xel si sobrepasa el umbral
- tipo se elige un tipo de umbralizaci칩n: THRESH_BINARY, THRESH_BINARY_INV, THRESH_TRUNC, THRESH_TOZERO], 
  THRESH_TOZERO_INV, THRESH_OTSU.

La funci칩n devuelve:
- thresh imagen binarizada
- ret valor del umbral

THRESH_BINARY
Y muestra que, si el p칤xel (src(x,y)) supera el umbral (thresh), en la imagen binarizada a los p칤xeles que superaron 
el umbral se les asigna el valor m치ximo establecido.

THRESH_BINARY_INV
si el p칤xel (src(x,y)) supera el umbral (thresh), en la imagen binarizada a los p칤xeles que superaron el umbral se les 
asigna cero 0 y a los que no superaron el umbral se les asigna el valor m치ximo establecido (maxval en este ejemplo es 
255)

THRESH_TRUNC
Estas muestran que, si el p칤xel (src(x,y)) supera el umbral (thresh), en la imagen binarizada a los p칤xeles que 
superaron el umbral se les asigna el mismo valor del umbral y a los que no superaron el umbral se les asigna los mismos
valores que ten칤an originalmente.

THRESH_TOZERO
si el p칤xel (src(x,y)) supera el umbral (thresh), en la imagen binarizada a los p칤xeles que superaron el umbral 
mantienen el valor de los pixeles originalmente, y cuando no superan el umbral se les asigna cero.

THRESH_TOZERO_INV
si el p칤xel (src(x,y)) supera el umbral (thresh), en la imagen binarizada a los p칤xeles que superaron el umbral se les 
asigna cero, y a los p칤xeles que no superaron el umbral se les asigna el mismo valor que originalmente ten칤as.
.'''
# Aplique un umbral global para crear una m치scara binaria del logotipo
retval, img_mask = cv2.threshold(img_gray, 127, 255, cv2.THRESH_BINARY)
plt.imshow(img_mask, cmap="gray")
plt.show()
print(img_mask.shape)

# **** Se invierte la m치scara
# Se cre una m치scara inversa
'''
Y luego vamos a realizar una operaci칩n similar aqu칤 abajo, pero sin usar la funci칩n de umbral.
Aunque podr칤amos haberlo hecho, podr칤amos haber usado la funci칩n de umbral aqu칤 abajo y especificar un umbral
m치scara inversa binaria:
retval2, img_mask2 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_BINARY_INV)

pero en su lugar podemos simplemente llamar a la funci칩n bitwise_not en la m치scara de imagen para devolver la m치scara 
inversa
'''
img_mask_inv = cv2.bitwise_not(img_mask)
plt.imshow(img_mask_inv, cmap="gray")
plt.show()

# **** Se aplica el fondo a la m치scara
'''para mostrar el fondo  "detr치s" de las letras del logotipo se utiliza bitwise_and usando 
la imagen de fondo consigo misma pero utilizando la m치scara original creada pero solo la va a aplicar a la m치scara, que 
es las letras blancas en este caso, es decir vamos a hacer una comparaci칩n bit a bit entre estas dos im치genes y el valor
devuelto de esa comparaci칩n ser치 el de la imagen si los p칤xeles correspondientes en ambas im치genes son iguales solo en 
las letras y en el resto 0 (negro), con esto obtenemos solo los colores que se muestran en el logotipo.'''
img_background = cv2.bitwise_and(img_background_rgb, img_background_rgb, mask=img_mask)
plt.imshow(img_background)
plt.show()

# **** Se a칤sla el primer plano de la imagen
'''A칤sle el primer plano (rojo de la imagen original) usando la m치scara inversa consigo misma y aplic치ndolo a la mascara
inversa con lo que se aplicar치 a toda la imagen la comparaci칩n de rojo = rojo menos a las letras, quedando 칠stas a 0
(negro)'''
img_foreground = cv2.bitwise_and(img_rgb, img_rgb, mask=img_mask_inv)
plt.imshow(img_foreground)
plt.show()

# **** Se obtiene el resultado
'''Ahora sumando las dos imagenes que acabamos de crear obtenemos el resultado del fondo rojo + las letras de colores'''
result = cv2.add(img_background, img_foreground)
plt.imshow(result)
cv2.imwrite("logo_final.png", result[:, :, ::-1])
plt.show()


import cv2
import sys

# ******************************************
# ***** Usando la camara en OpenCV
# ******************************************

# especificamos un 칤ndice de dispositivo de c치mara predeterminado de cero.
s = 0
print(sys.argv)  # contiene los argumentos de la librer칤a sys, por ejemplo 0 es la ruta
# ['C:\\Users\\jgomcano\\PycharmProjects\\guiapython\\OpenCV\\Usando la camara en openCV\\Usando_camara_OpenCV.py']
# y simplemente estamos verificando si hubo una especificaci칩n de l칤nea de comando para anular ese valor predeterminado.
if len(sys.argv) > 1:
    s = sys.argv[1]
print(s)  # 0
source = cv2.VideoCapture(s)  # llamamos a la clase de captura de video para crear un objeto de captura de video,
#  Con el 칤ndice 0 acceder치 a la c치mara predeterminada en su sistema, si no hay que indicarlo
win_name = 'Vista de camara'
# estamos creando una ventana con nombre, que eventualmente vamos a enviar la salida transmitida
cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)

'''ciclo while nos permitir치 transmitir continuamente video desde la c치mara y enviarlo a la salida a menos que el 
usuario pulse la tecla de escape.'''
while cv2.waitKey(1) != 27:  # Escape
    '''usa esa fuente de objeto de captura de v칤deo  de captura de video para llamar al m칠todo read, que  devolver치 un 
    solo cuadro de la transmisi칩n de video, as칤 como una variable l칩gica has_frame.
    Entonces, si hay alg칰n tipo de problema con la lectura de la transmisi칩n de video o el acceso a la c치mara, entonces 
    has_frame ser칤a falso y saldr칤amos del bucle.
    De lo contrario, continuar칤amos y llamar칤amos a la funci칩n de visualizaci칩n de mensajes instant치neos y abrir칤amos
     kbps para enviar el video (frame) a la ventana de salida'''
    has_frame, frame = source.read()
    if not has_frame:
        break
    cv2.imshow(win_name, frame)

source.release()
cv2.destroyWindow(win_name)


import cv2
import matplotlib.pyplot as plt
# %matplotlib inline


# ******************************************
# ***** Escribir video en el disco
# ******************************************


source = './race_car.mp4'  # source = 0 for webcam
cap = cv2.VideoCapture(source)  # llamamos a la clase de captura de video para crear un objeto de captura de video,

# Comprobamos si se cre칩 correctamente el objeto y est치 abierto
if (cap.isOpened()== False):
  print("Error opening video stream or file")

# ****** Leer y mostrar un frame
'''
Los 3 puntos ... son una Ellipsis en Pyhton significan que puedes recibir los que sea y ya el ultimo valor (en este caso
 el ultimo array) , es iterar d치ndole la vuelta empezando por el ultimo para darle la vuelta a los canales,  ser칤a igual
que [:,:, ::-1]
https://realpython.com/python-ellipsis/
'''
ret, frame = cap.read()
plt.imshow(frame[..., ::-1])
plt.show()

# Mostrar el video del archivo en jupyter
# from IPython.display import HTML
# HTML("""
# <video width=1024 controls>
#   <source src="race_car.mp4" type="video/mp4">
# </video>
# """)

# **** Mostrar el video del archivo desde consola mediante ventana
#
# win_name = 'video'
# # estamos creando una ventana con nombre, que eventualmente vamos a enviar la salida transmitida
# cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)
#
# '''ciclo while nos permitir치 transmitir continuamente video desde la c치mara y enviarlo a la salida a menos que el
# usuario pulse la tecla de escape.'''
# while cv2.waitKey(1) != 27:  # Escape
#     '''usa esa fuente de objeto de captura de v칤deo  de captura de video para llamar al m칠todo read, que  devolver치 un
#     solo cuadro de la transmisi칩n de video, as칤 como una variable l칩gica has_frame.
#     Entonces, si hay alg칰n tipo de problema con la lectura de la transmisi칩n de video o el acceso a la c치mara,
#     has_frame ser칤a falso y saldr칤amos del bucle.
#     De lo contrario, continuar칤amos y llamar칤amos a la funci칩n de visualizaci칩n de mensajes instant치neos y abrir칤amos
#      kbps para enviar el video (frame) a la ventana de salida'''
#     has_frame, frame = cap.read()
#     if not has_frame:
#         break
#     cv2.imshow(win_name, frame)
#
# cap.release()
# cv2.destroyWindow(win_name)


# **** Escribir el v칤deo usando OpenCV ( ojo con no haber ya recorrido el objeto de video )
'''
Para escribir el video, debe crear un objeto de videowriter con los par치metros correctos.

Sintaxis de la funci칩n
VideoWriter objeto = cv.VideoWriter (nombre de archivo, fourcc, fps, frameSize)
Par치metros
-filename: Nombre del archivo de v칤deo de salida.
-fourcc: c칩digo de c칩dec de 4 caracteres que se utiliza para comprimir los fotogramas.
 Por ejemplo, VideoWriter::fourcc('P','I','M','1') es un c칩dec MPEG-1, VideoWriter::fourcc('M','J','P','G ') es un c칩dec
 jpeg de movimiento, etc. La lista de c칩digos se puede obtener en la p치gina Video Codecs by FOURCC. El backend FFMPEG 
 con contenedor MP4 usa de forma nativa otros valores como c칩digo fourcc: consulte ObjectType, por lo que puede recibir 
 un mensaje de advertencia de OpenCV sobre la conversi칩n del c칩digo fourcc.
- fps: velocidad de fotogramas de la transmisi칩n de video creada.
- frameSize: Tama침o de los fotogramas de v칤deo tupla (ancho,alto).

*El tama침o del marco es importante porque deben ser las dimensiones de los marcos que tiene en la memoria que desea 
 escribir en el disco


Lo primero que vamos a hacer es usar el objeto de captura de video para llamar a este m칠todo de get(), que
nos va a recuperar las dimensiones del cuadro de video que tenemos en memoria.'''
# Se obtienen las resoluciones predeterminadas del cuadro, int() Convierte las resoluciones de float a entero
frame_width = int(cap.get(3))  # en 3 guarda el ancho
frame_height = int(cap.get(4))  # en 4 guarda el alto

# Define el c칩dec y crea el objeto VideoWriter.
out_avi = cv2.VideoWriter('race_car_out.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))
out_mp4 = cv2.VideoWriter('race_car_out.mp4',cv2.VideoWriter_fourcc(*'XVID'), 10, (frame_width,frame_height))

# Leer fotogramas y escribir en el archivo
'''Leeremos los cuadros del video del auto de carreras y escribiremos lo mismo en los dos objetos que creamos en el paso
 anterior. Deber칤amos liberar los objetos despu칠s de completar la tarea.'''

# leer mientras el video se completa
while (cap.isOpened()):
  # Va capturando frame a frame
  ret, frame = cap.read()

  if ret == True:

    # Escribe cada frame en los ficheros
    out_avi.write(frame)
    out_mp4.write(frame)

  # rompe el bucle
  else:
    break

# Cuando todo est칠 listo, liberamos los objetos VideoCapture y VideoWriter
cap.release()
out_avi.release()
out_mp4.release()

import cv2
import sys
import numpy




# **********************************
# ***** Filtrado de imagen en OpenCV
# **********************************

PREVIEW  = 0   # Vista previa
BLUR     = 1   # filtro de desenfoque
FEATURES = 2   # Detector de caracter칤sticas de corner
CANNY    = 3   # Detector de borde astuto

# Estamos definiendo un peque침o diccionario de configuraci칩n de par치metros para el detector de caracter칤sticas de corner
feature_params = dict( maxCorners = 500,
                       qualityLevel = 0.2,
                       minDistance = 15,
                       blockSize = 9)

'''Estamos configurando el 칤ndice del dispositivo para la c치mara (linea22), creando una ventana de salida para los 
resultados transmitidos (30)y luego crea un objeto de captura de video (33) para que podamos procesar la transmisi칩n de 
video en el bucle (36)'''
s = 0
if len(sys.argv) > 1:
    s = sys.argv[1]

image_filter = PREVIEW
alive = True

win_name = 'Camera Filters'
cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)
result = None

source = cv2.VideoCapture(s)

while alive:
    has_frame, frame = source.read()  # leemos el frame de v칤deo
    if not has_frame:
        break

    frame = cv2.flip(frame,1)  # mediante flip giramos el video horizontalmente

    if image_filter == PREVIEW:  # seg칰n la configuraci칩n de ejecuci칩n del script (l칤nea 27)
        result = frame  # solo cogemos el frame y lo mostramos
    elif image_filter == CANNY:
        '''
        Detector de bordes Canny con OpenCV
        La funci칩n Canny() en OpenCV se utiliza para detectar los bordes de una imagen
        canny = cv2.Canny(imagen, umbral_minimo, umbral_maximo)
        Donde:
        - canny: es la imagen resultante. Aparecer치n los bordes detectados tras el proceso.
        - imagen: es la imagen original.
        - umbral_minimo: es el umbral m칤nimo en la umbralizaci칩n por hist칠resis
        - umbral_maximo: es el umbral m치ximo en la umbralizaci칩n por hist칠resis
        hay mas par치metros: 
        - opening_size: Tama침o de apertura del filtro Sobel.
        - L2Gradient: Par치metro booleano utilizado para mayor precisi칩n en el c치lculo de Edge Gradient.
        el umbral m칤nimo y el m치ximo depender치 de cada situaci칩n.
        Docu 
        https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html'''
        result = cv2.Canny(frame, 80, 150)
    elif image_filter == BLUR:
        '''
        m칠todo cv2.blur()
        El m칠todo se utiliza para desenfocar una imagen utilizando el filtro de cuadro normalizado. La funci칩n suaviza 
        una imagen.
        Sintaxis: cv2.blur(src, ksize[, dst[, ancla[, borderType]]])
        Par치metros:
        - src: Es la imagen de la que se desea difuminar.
        - ksize: una tupla que representa el tama침o del kernel de desenfoque, es decir  son las dimensiones del n칰cleo 
            de la caja. En este ejemplo ser칤a un kernel de caja de 13 por 13 que estar칤a involucrado con la imagen para 
            dar como resultado una imagen borrosa. si el tama침o del kernel es m치s peque침o que el desenfoque, se reduce, 
            si el tama침o del kernel es m치s grande se obtiene un desenfoque m치s sustancial.
        - dst: Es la imagen de salida del mismo tama침o y tipo que src.
        - ancla: es una variable de tipo entero que representa el punto de anclaje y su valor predeterminado es (-1, -1)
          ,lo que significa que el ancla est치 en el centro del kernel.
        - borderType: representa qu칠 tipo de borde se agregar치. Est치 definido por indicadores como cv2.BORDER_CONSTANT 
          , cv2.BORDER_REFLECT , etc.
        - Valor devuelto: Devuelve una imagen.
        '''
        result = cv2.blur(frame, (13, 13))
    elif image_filter == FEATURES:
        result = frame
        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # convertimos la imagen a escala de grises
        '''La funci칩n goodFeaturesToTrack encuentra N esquinas m치s fuertes 
         cv2.goodFeaturesToTrack(image, maxCorners, qualityLevel, minDistance, [,mask[,blockSize[,useHarrisDetector[,k]]]])

        - imagen: entrada de imagen de un solo canal de 8 bits o punto flotante de 32 bits
        - maxCorners - N칰mero m치ximo de esquinas a devolver. Si hay m치s esquinas de las que se encuentran, se devuelve 
          la m치s fuerte de ellas. si <= 0 implica que no se establece ning칰n l칤mite en el m치ximo y se devuelven todas 
          las esquinas detectadas.
        - qualityLevel - Par치metro que caracteriza la calidad m칤nima aceptada de las esquinas de la imagen. Consulte el 
          p치rrafo anterior para obtener una explicaci칩n.
        - minDistance - Distancia euclidiana m칤nima posible entre las esquinas devueltas
        - m치scara - Regi칩n de inter칠s opcional. Si la imagen no est치 vac칤a, especifica la regi칩n en la que se detectan 
          las esquinas.
        - blockSize - Tama침o de un bloque promedio para calcular una matriz de covariaci칩n derivada sobre cada 
          vecindario de p칤xeles
        - useHarrisDetector - ya sea para usar Shi-Tomasi o Harris Corner
        -k - Par치metro libre del detector de Harris
        https://theailearner.com/tag/cv2-goodfeaturestotrack/'''
        corners = cv2.goodFeaturesToTrack(frame_gray, **feature_params)
        if corners is not None:  # Devuelve una lista de esquinas encontradas en la imagen
            ''' Y si detectamos una o m치s esquinas, simplemente anotaremos el resultado con peque침os
             c칤rculos verdes para indicar las ubicaciones de esas caracter칤sticas ojo con los par치metros
             al ser capturados las  posiciones x,y PASARLO a entero'''
            for x, y in numpy.float32(corners).reshape(-1, 2):
                cv2.circle(result, (int(x), int(y)), 10, (0, 255 , 0), 1)

        cv2.imshow(win_name, result)  # Enviamos el resultado a la salida

        # para poder cambiar el tratamiento de la imagen dependiendo de la tecla introducida
        key = cv2.waitKey(1)
        if key == ord('Q') or key == ord('q') or key == 27:
            alive = False
        elif key == ord('C') or key == ord('c'):
            image_filter = CANNY
        elif key == ord('B') or key == ord('b'):
            image_filter = BLUR
        elif key == ord('F') or key == ord('f'):
            image_filter = FEATURES
        elif key == ord('P') or key == ord('p'):
            image_filter = PREVIEW

    source.release()
    cv2.destroyWindow(win_name)

import cv2
import numpy as np
import matplotlib.pyplot as plt

# ************************************************************
# ***** Caracteristicas de la imagen y alineaci칩n de la imagen
# ************************************************************
'''
Demostraremos los pasos a trav칠s de un ejemplo en el que alinearemos una foto de un formulario tomado con un tel칠fono 
m칩vil con una plantilla del formulario. La t칠cnica que usaremos a menudo se denomina alineaci칩n de im치genes "basada en 
funciones" porque en esta t칠cnica se detecta un conjunto escaso de funciones en una imagen y se compara con las 
funciones en la otra imagen. Luego se calcula una transformaci칩n basada en estas caracter칤sticas combinadas que deforma 
una imagen sobre la otra.

La alineaci칩n de im치genes (tambi칠n conocida como registro de im치genes) es la t칠cnica de deformar una imagen (o, a veces,
ambas im치genes) para que las caracter칤sticas de las dos im치genes se alineen perfectamente.
'''

# **** Paso 1: Lea la plantilla y la imagen escaneada
# Leemos la imagen de referencia
refFilename = "form.jpg"
print("Reading reference image : ", refFilename)
im1 = cv2.imread(refFilename, cv2.IMREAD_COLOR)
im1 = cv2.cvtColor(im1, cv2.COLOR_BGR2RGB)

# leemos la imagen que queremos alinear
imFilename = "scanned-form.jpg"
print("Reading image to align : ", imFilename)
im2 = cv2.imread(imFilename, cv2.IMREAD_COLOR)
im2 = cv2.cvtColor(im2, cv2.COLOR_BGR2RGB)

# Mostramos las im치genes cargadas
plt.figure(figsize=[20, 10]);
plt.subplot(121);
plt.axis('off');
plt.imshow(im1);
plt.title("Original Form")
plt.subplot(122);
plt.axis('off');
plt.imshow(im2);
plt.title("Scanned Form")
plt.show()

# ****** Paso 2: encuentra puntos clave en ambas im치genes
'''
objetivo  es tratar de extraer informaci칩n significativa que est칠 contextualmente relacionada con la imagen en s칤.
Por lo general, buscamos bordes, esquinas y texturas en las im치genes, las funci칩n orb() es una forma de hacerlo, 
vamos a crear este objeto orbe, y luego vamos a usar ese objeto para detectar y calcular puntos clave y descriptores 
para cada una de las im치genes.

Entonces, los puntos clave son caracter칤sticas interesantes en cada imagen que generalmente se asocian con algunos 
puntos n칤tidos. borde o esquina, y est치n descritos por un conjunto de coordenadas de p칤xeles que describen la ubicaci칩n
del punto clave. El tama침o del punto clave. En otras palabras, la escala del punto clave y luego tambi칠n la orientaci칩n 
del punto clave. luego hay una lista asociada de descriptores para cada punto clave, y cada descriptor es en realidad un
vector de alguna informaci칩n que describe la regi칩n alrededor del punto clave, que act칰a efectivamente como una firma 
para ese punto clave. Es una representaci칩n vectorial de la informaci칩n de p칤xeles alrededor del punto clave. Y la idea 
aqu칤 es que si estamos buscando el mismo punto clave en ambas im치genes, podemos intentar usar los descriptores para 
emparejarlos.'''

# Convertimos las im치genes a escala de grises
im1_gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)
im2_gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)

# Detecta caracter칤sticas de ORB y calcula descriptores.

MAX_NUM_FEATURES = 500
'''El algoritmo utilizado para la detecci칩n de caracter칤sticas de la imagen dada junto con la orientaci칩n y los 
descriptores de la imagen se denomina algoritmo ORB y es una combinaci칩n del detector de punto clave FAST y el 
descriptor BRIEF.

- Localizador : identifica puntos en la imagen que son estables bajo transformaciones de imagen como traslaci칩n 
  (desplazamiento), escala (aumento/disminuci칩n de tama침o) y rotaci칩n. El localizador encuentra las coordenadas x, y de 
  dichos puntos. El localizador que utiliza el detector ORB se llama FAST .
- Descriptor : El localizador del paso anterior solo nos dice d칩nde est치n los puntos interesantes. La segunda parte del 
  detector de caracter칤sticas es el descriptor que codifica la apariencia del punto para que podamos distinguir un punto
  caracter칤stico de otro. El descriptor evaluado en un punto caracter칤stico es simplemente una matriz de n칰meros. 
  Idealmente, el mismo punto f칤sico en dos im치genes deber칤a tener el mismo descriptor. ORB usa una versi칩n modificada 
  del descriptor de caracter칤sticas llamado BRISK .

sintaxis 
ORB_object = cv.ORB_create()
keypoints = ORB_object.detect(input_image)
keypoints, descriptors = ORB_object.compute(input_image, keypoints)

- El algoritmo ORB se puede implementar usando una funci칩n llamada funci칩n ORB().
- La implementaci칩n del algoritmo ORB funciona creando un objeto de la funci칩n ORB().
- Luego hacemos uso de una funci칩n llamada funci칩n ORB_object.detect() para detectar los puntos clave de una imagen dada
- Luego hacemos uso de una funci칩n llamada funci칩n ORB_object.compute() para calcular los descriptores de una imagen 
  determinada.
- Luego, la imagen con los puntos clave calculados dibujados en la imagen se devuelve como salida
https://www.educba.com/opencv-orb/
https://learnopencv.com/image-alignment-feature-based-using-opencv-c-python/


'''
orb = cv2.ORB_create(MAX_NUM_FEATURES)

# detectAndCompute a칰na las dos explicadas anteriormente
keypoints1, descriptors1 = orb.detectAndCompute(im1_gray, None)
keypoints2, descriptors2 = orb.detectAndCompute(im2_gray, None)

'''Estamos dibujando los puntos clave detectados en la imagen usando la funci칩n drawKeypoints()
Sintaxis de la funci칩n drawKeypoints():
dibujar puntos clave (imagen_de_entrada, puntos_clave, imagen_de_salida, color, bandera)
par치metros:
- input_image: la imagen que se convierte en escala de grises y luego los puntos clave se extraen utilizando los 
                algoritmos SURF o SIFT se denomina imagen de entrada.
- key_points: los puntos clave obtenidos de la imagen de entrada despu칠s de usar los algoritmos se denominan puntos 
              clave.
- output_image :   imagen sobre la que se dibujan los puntos clave.
- color : el color de los puntos clave.
- bandera: las caracter칤sticas del dibujo est치n representadas por la bandera.
https://www.geeksforgeeks.org/python-opencv-drawkeypoints-fuction/
'''
im1_display = cv2.drawKeypoints(im1, keypoints1, outImage=np.array([]), color=(255, 0, 0),
                                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
im2_display = cv2.drawKeypoints(im2, keypoints2, outImage=np.array([]), color=(255, 0, 0),
                                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

'''
Hemos calculado los puntos clave calculados en los descriptores de cada imagen. Y aqu칤, en estas cifras, se muestran 
solo los puntos clave.  todos estos c칤rculos rojos son puntos clave. El centro del c칤rculo es la ubicaci칩n del punto 
clave. El tama침o del c칤rculo representa la escala del punto clave y luego la l칤nea que conecta el centro del c칤rculo al 
exterior del c칤rculo representa la orientaci칩n del punto clave. Hay algunos puntos clave en ambas im치genes que tal vez 
sean los mismos, y esos son los que vamos a tratar de encontrar para que podamos calcular el gr치fico de Hamas entre 
estas dos representaciones de im치genes.'''

plt.figure(figsize=[20, 10]);
plt.subplot(121);
plt.axis('off');
plt.imshow(im1_display);
plt.title("Original Form")
plt.subplot(122);
plt.axis('off');
plt.imshow(im2_display);
plt.title("Scanned Form")
plt.show()

# **** Paso 3: haga coincidir los puntos clave en las dos im치genes
'''
El primer paso en este proceso de coincidencia es crear una coincidencia u objeto llamando a DescriptorMatcher_create.
le pasamos a esa funci칩n DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING, una medida de distancia (los descriptores de o cadena 
binaria requieren una m칠trica de hamming para ese objetivo). lo que hace es  Toma el descriptor de una caracter칤stica 
en el primer conjunto y se compara con todas las dem치s caracter칤sticas en el segundo conjunto utilizando alg칰n c치lculo 
de distancia. Y se devuelve el m치s cercano.

luego usamos esa coincidencia u objeto para llamar a la funci칩n de match, que luego intenta proporcionar una lista de 
las mejores coincidencias asociadas con esa lista de descriptores. tenemos una estructura de datos  que contiene la 
lista de coincidencias de los puntos clave que determinamos arriba.

Y luego, una vez que obtengamos esa lista, ordenaremos la lista en funci칩n de la distancia entre los distintos, tras lo 
que vamos a limitar al 10 por ciento superior de las coincidencias devueltas.

https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html
'''
# Coincidir las caracter칤sticas encontradas en ambas im치genes.
matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)

# match ()para obtener las mejores coincidencias en dos im치genes.
matches = matcher.match(descriptors1, descriptors2, None)
'''
el resutlado de la l칤nea l칤nea 162 es una lista de objetos DMatch. Este objeto DMatch tiene los siguientes atributos:
DMatch.distance - Distancia entre descriptores. Cuanto m치s bajo, mejor.
DMatch.trainIdx - 칈ndice del descriptor en descriptores de train
DMatch.queryIdx - 칤ndice del descriptor en los descriptores de consulta
DMatch.imgIdx - 칈ndice de la imagen de train.
'''
# ordenar las coincidencias por resultado ascendentemente
matches = sorted(matches, key=lambda x: x.distance, reverse=False)  # al ser una tupla sort no.

# Eliminar las coincidencias menos favorables, qued치ndonos solo con el 10%
numGoodMatches = int(len(matches) * 0.1)
matches = matches[:numGoodMatches]

'''
Y vamos a usar DrewMatches para dibujar las coincidencias en este c칩digo, puedes ver que varios puntos clave en una 
imagen coinciden los puntos clave de la otra imagen'''
# Dibujar las mejores coincidencias aportando las dos im치genes, sus puntos y las coincidencias
im_matches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)

plt.figure(figsize=[40, 10])
plt.imshow(im_matches);
plt.axis('off');
plt.title("Original Form");
plt.show()

# **** Paso 4: Encuentra la homograf칤a
'''
쯈u칠 es la Homograf칤a?
Considere dos im치genes de un plano con un libro en diferentes posiciones y distancia.  Si el libro tiene un cuadro con 
una imagen, un punto en la esquina del cuadro representa el mismo punto en las dos im치genes. En la jerga de la visi칩n 
artificial, llamamos a estos puntos correspondientes. Una homograf칤a es una transformaci칩n (una matriz de 3칑3) que 
asigna los puntos de una imagen a los puntos correspondientes de la otra imagen.

Si conoci칠ramos la homograf칤a, podr칤amos aplicarla a todos los p칤xeles de una imagen para obtener una imagen 
deformada que est칠 alineada con la segunda imagen, es decir , puede aplicar la homograf칤a a la primera imagen y el libro
de la primera imagen se alinear치 con el libro de la segunda imagen. Si conocemos 4 o m치s puntos correspondientes en las
dos im치genes, podemos usar la funci칩n de OpenCV findHomography para encontrar la homograf칤a

h, status = cv2.findHomography(points1, points2)
donde, puntos1 y puntos2 son vectores/matrices de puntos correspondientes, y h es la matriz homogr치fica.'''

# Extraer ubicaci칩n de las buenas coincidencias
'''Crea y devuelve una referencia a un array con las dimensiones especificadas en la tupla dimensiones cuyos elementos 
son todos ceros b치sicamente est치 creando un array de arrays con los puntos inicializados a 0, con el n칰mero de puntos 
por la longitud que tiene el objeto matches
'''
points1 = np.zeros((len(matches), 2), dtype=np.float32)
points2 = np.zeros((len(matches), 2), dtype=np.float32)

'''Recorre matches desde la primera posici칩n va introducendo el valor de los puntos de los descriptores de match de 
entrenamiento y consulta'''
for i, match in enumerate(matches):
    points1[i, :] = keypoints1[match.queryIdx].pt
    points2[i, :] = keypoints2[match.trainIdx].pt

# Encuentra la homograf칤a
h, mask = cv2.findHomography(points2, points1, cv2.RANSAC)

# ***** Paso 5: deformar la imagen
# Usar homograf칤a para deformar la imagen
height, width, channels = im1.shape  # desmpaquetamos la dimensi칩n de la imagen de referencia

''' la transformaci칩n de perspectva est치 asociada con el cambio de punto de vista. Este tipo de transformaci칩n no
conserva el paralelismo, la longitud y el 치ngulo pero conserva la colinealidad y la incidencia, lo que significa que 
las l칤neas rectas permanecer치n rectas despues de la transformaci칩n. 

para ello seleccionamos 4 puntos de la imagen de entrada y asignamos esos 4 puntos a las ubicaciones deseadas en la 
imagen de salida, realizando

dst = cv.warpPerspective(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]] )
# src: imagen de entrada
# M: Matriz de transformaci칩n, en este caso usamos la homograf칤a como esa matriz
# dsize: tama침o de la imagen de salida (ancho, alto)
# flags: m칠todo de interpolaci칩n a utilizar
https://theailearner.com/tag/cv2-warpperspective/'''

im2_reg = cv2.warpPerspective(im2, h, (width, height))
# Display results
plt.figure(figsize=[20, 10]);
plt.subplot(121);
plt.imshow(im1);
plt.axis('off');
plt.title("Original Form");
plt.subplot(122);
plt.imshow(im2_reg);
plt.axis('off');
plt.title("Scanned Form");
plt.show()



import cv2
import glob
import matplotlib.pyplot as plt
import math

# ***********************************************
# ***** Uni칩n de im치genes y creaci칩n de panoramas
# ***********************************************
# Caracteristicas de la imagen y alineaci칩n de la imagen

# Creando panoramas usando OpenCV
'''
1. Encuentra puntos clave en todas las im치genes
2. Encuentra correspondencias por pares
3. Estimar homograf칤as por pares
4. Refinar homograf칤as
5. Puntada con mezcla

podemos realizar todos estos pasos con la clase stitcher, es muy similar a los pasos que se explican en  
Caracteristicas de la imagen y alineaci칩n de la imagen. stitcher es una clase que nos permite crear panoramas 
simplemente pasando una lista de im치genes.

las im치genes utilizadas para crear panoramas deben tomarse desde el mismo punto de vista Y tambi칠n es importante tomar 
las fotos aproximadamente al mismo tiempo para minimizar la iluminaci칩n.
'''

# Leemos las im치genes,
'''glob incluye funciones para buscar en una ruta todos los nombres de archivos y/o directorios que coincidan con un 
determinado patr칩n 
glob.glob() devuelve una lista con las entradas que coincidan con el patr칩n especificado en pathname.
glob.glob(pathname, recursive=False)
La b칰squeda se puede hacer tambi칠n recursiva con el argumento recursive=True y las rutas pueden ser absolutas 
y relativas.'''
imagefiles = glob.glob("boat/*")
imagefiles.sort()  # ordenamos la lista obtenida
# ['boat\\boat1.jpg', 'boat\\boat2.jpg', 'boat\\boat3.jpg', 'boat\\boat4.jpg', 'boat\\boat5.jpg', 'boat\\boat6.jpg']

images = []
# recorremos la lista de imagenes y para cada imagen la leemos en color a침adiendo los objeto a una lista de im치genes
for filename in imagefiles:
  img = cv2.imread(filename)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  images.append(img)

num_images = len(images)

# mostramos las im치genes
plt.figure(figsize=[30,10])
num_cols = 3
num_rows = math.ceil(num_images / num_cols)
for i in range(0, num_images):
  plt.subplot(num_rows, num_cols, i+1)
  plt.axis('off')
  plt.imshow(images[i])
plt.show()

# Stitch Images
'''
Creamos un objeto Stitcher desde la clase Stitcher_create(). Usamos ese objeto para llamar al m칠todo de stitch y 
simplemente pasamos una lista de im치genes. el resultado que obtenemos es la imagen panor치mica.
El panorama de retorno incluye estas regiones negras. aqu칤, que son el resultado de la deformaci칩n que se requiri칩 para 
 unir las im치genes.'''
stitcher = cv2.Stitcher_create()
status, result = stitcher.stitch(images)
if status == 0:
  plt.figure(figsize=[30,10])
  plt.imshow(result)
plt.show()


import cv2
import sys
import os
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from IPython.display import HTML
import urllib
import zipfile
# ****************************
# ***** Seguimiento de Objetos
# ****************************
'''Objetivo: dada la ubicaci칩n inicial de un objeto, realizar un seguimiento de la ubicaci칩n en fotogramas posteriores.

El seguimiento generalmente se refiere a estimar la ubicaci칩n de un objeto y predecir su ubicaci칩n en alg칰n momento
futuro en el tiempo, y en el contexto de la visi칩n por computadora, generalmente equivale a detectar un objeto de
inter칠s en un video para posteriormente predecir la ubicaci칩n de ese objeto en cuadros de video subsiguientes Y logramos
esto mediante el desarrollo de un modelo de movimiento y un modelo de apariencia, usando esa informaci칩n para predecir
su ubicaci칩n y futuros cuadros de video.

Tambi칠n podemos usar un modelo de apariencia que codifica el aspecto del objeto y buscar la regi칩n alrededor de la
ubicaci칩n predicha del modelo de movimiento para ajustar la ubicaci칩n del objeto. El modelo de movimiento es una
aproximaci칩n a la ubicaci칩n del objeto en un cuadro de video futuro, y  se usa el modelo de apariencia para afinar esa
estimaci칩n.

Como un ejemplo concreto, supongamos que estamos interesados en rastrear un objeto espec칤fico como el coche de carreras
identificado en el primer fotograma de un videoclip. Para iniciar el algoritmo de seguimiento, necesitamos especificar 
la ubicaci칩n inicial del objeto y para hacer esto, definimos un cuadro delimitador que se muestra aqu칤 en azul, que 
consta de dos conjuntos de coordenadas de p칤xeles que definen las esquinas superior izquierda e inferior derecha del 
cuadro delimitador. uUna vez que el algoritmo de seguimiento se inicializa con esta informaci칩n, el objetivo es realizar
un seguimiento del objeto y los cuadros de video subsiguientes al producir un cuadro delimitador en cada nuevo cuadro de
video.

En OpenCV tenemos 8 algoritmos de seguimiento disponibles:
1. BOOSTING
2. MIL
3. KCF
4. CRST
5. TLD -> Tiende a recuperarse de las oclusiones.
6. MEDIANFLOW -> Bueno para c치mara lenta predecible
7. GOTRUN -> Basado en aprendizaje profundo, M치s preciso
8. MOSSE -> El m치s r치pido
'''
video_input_file_name = "race_car.mp4"

# *** Definici칩n de funciones


def drawRectangle(frame, bbox):  # Cuadro delimitador, dibujar
    p1 = (int(bbox[0]), int(bbox[1]))  # punto izquierdo superior
    p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))  # punto inferior derecho
    cv2.rectangle(frame, p1, p2, (255, 0, 0), 2, 1)
    # imagen, v칠rtice sup izq, v칠rtice inf der, color(R,G,B), grosor, tipo de l칤nea


def displayRectangle(frame, bbox):  # Cuadro delimitador, mostrar
    plt.figure(figsize=(20, 10))
    frameCopy = frame.copy()  # copiamos el fotograma
    drawRectangle(frameCopy, bbox)  # Llamamos al de arriba para dibujar el rect치ngulo en el fotograma
    frameCopy = cv2.cvtColor(frameCopy, cv2.COLOR_RGB2BGR)  # cambio de color
    plt.imshow(frameCopy); plt.axis('off')  # mostramos el fotograma


def drawText(frame, txt, location, color = (50,170,50)):
    cv2.putText(frame, txt, location, cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)  # dibujamos texto en el fotograma

'''Uno de los algoritmos es el modelo GOTURN que requiere un modelo de inferencia, que se entrena teniendo como entrada
el fotograma previo el actual, pasa por el modelo de red neuronal entrenado ( conocido como modelo de inferencia) 
Utiliza el cuadro delimitador del cuadro anterior para recortar ambas im치genes y, por lo tanto, el objeto de inter칠s se 
encuentra en el centro de este cuadro anterior. Y obviamente, si el objeto se ha movido en el marco actual, entonces no 
estar치 centrado en este recorte de fotograma porque estamos usando el cuadro delimitador del cuadro anterior para 
recortar ambos fotogramas. Y luego es el trabajo del modelo de inferencia predecir cu치l es el cuadro delimitador en la 
salida y da como salida el fotograma de seguimiento actual.
'''
# Descargar modelo de seguimiento (solo  GOTURN)
if not os.path.isfile('goturn.prototxt') or not os.path.isfile('goturn.caffemodel'):
    print("Downloading GOTURN model zip file")
    urllib.request.urlretrieve('https://www.dropbox.com/sh/77frbrkmf9ojfm6/AACgY7-wSfj-LIyYcOgUSZ0Ua?dl=1',
                               'GOTURN.zip')

    # descomprimir el fichero
    '''
    El m칠todo extractall() se usa para extraer paratodos los archivos presentes en el archivo zip al directorio de trabajo 
    actual. Los archivos tambi칠n se pueden extraer a una ubicaci칩n diferente sin pasar por el par치metro de ruta.
    sintaxis: ZipFile.extractall(ruta_archivo, miembros=Ninguno, pwd=Ninguno)
    Par치metros:
    - file_path: ubicaci칩n donde se debe extraer el archivo comprimido, si file_path es None, el contenido del archivo zip se extraer치 al directorio de trabajo actual
    - miembros: Especifica la lista de archivos a extraer, si no se especifica, se extraer치n todos los archivos del zip. los miembros deben ser un subconjunto de la lista devuelta por namelist()
    - pwd: la contrase침a utilizada para los archivos cifrados. Por defecto, pwd es Ninguno.
    '''
    with zipfile.ZipFile("GOTURN.zip", 'r') as zObject:
        # Extracting all the members of the zip
        # into a specific location.
        zObject.extractall(
            path=None)
    # Delete the zip file
    os.remove('GOTURN.zip')

# **** Crear la instancia de Tracker
# Configurar rastreador definiendo una lista de tracker ( "rastreadores") disponibles en la API
tracker_types = ['BOOSTING', 'MIL','KCF', 'CSRT', 'TLD', 'MEDIANFLOW', 'GOTURN','MOSSE']

# Cambiar el 칤ndice para cambiar el tipo de rastreador
tracker_type = tracker_types[6]

if tracker_type == 'BOOSTING':
    tracker = cv2.legacy_TrackerBoosting.create()
elif tracker_type == 'MIL':
    tracker = cv2.TrackerMIL_create()
elif tracker_type == 'KCF':
    tracker = cv2.TrackerKCF_create()
elif tracker_type == 'CSRT':
    tracker = cv2.legacy_TrackerCSRT.create()
elif tracker_type == 'TLD':
    tracker = cv2.legacy_TrackerTLD.create()
elif tracker_type == 'MEDIANFLOW':
    tracker = cv2.legacy_TrackerMedianFlow.create()
elif tracker_type == 'GOTURN':
    tracker = cv2.TrackerGOTURN_create()
else:
    tracker = cv2.legacy_TrackerMOSSE.create()

# ***** Leer video de entrada y configuraci칩n de salida de video

# Leer video
'''# Estamos configurando las transmisiones de video de salida de entrada, por lo que pasamos la entrada de v칤deo (el 
nombre de archivo) y creando un objeto de entrada de v칤deo'''
video = cv2.VideoCapture(video_input_file_name)
ok, frame = video.read()  # leemos el primer fotograma del archivo
# plt.imshow(frame[..., ::-1])
# plt.show()
# Salir si no se puede abrir el video
if not video.isOpened():
    print("Could not open video")
    sys.exit()
else:
    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))  # capturamos del fotograma el ancho
    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))  # capturamos del fotograma el alto

video_output_file_name = 'race_car-' + tracker_type + '.mp4'  # nombre parametrizado del archivo
'''Para escribir el v칤deo, creamos un objeto de salida de v칤deo que escriba los resultados del algoritmo de seguimiento 
escogido 
* explicado en "Escribir video en el disco"
'''
video_out = cv2.VideoWriter(video_output_file_name, cv2.VideoWriter_fourcc(*'avc1'), 10, (width, height))

# ****** Definir cuadro delimitador
'''Necesit치bamos encontrar un cuadro delimitador alrededor del objeto que nos interesa rastrear, y lo estamos logrando 
aqu칤 de forma manual, Pero en la pr치ctica, seleccionar칤a eso con una interfaz de usuario o tal vez usar칤a un algoritmo 
de detecci칩n para detectar objetos de inter칠s para el seguimiento '''
bbox = (1300, 405, 160, 120)  # Dos conjuntos de pixeles, esquina sup izq y esquina inf der
# bbox = cv2.selectROI(frame, False)
# print(bbox)
displayRectangle(frame,bbox)

# ****** Inicializar rastreador
'''
Inicializamos el rastreador y para ello llamamos a tracker.init p치sandole el primer fotograma y el cuadro delimitador'''
ok = tracker.init(frame, bbox)

# ***** Marco de lectura y objeto de seguimiento
while True:
    '''Comprobamos que existe el objeto inicializado de tracker (ok) y el fotograma, adem치s Est치 leyendo el siguiente 
    fotograma del v칤deo'''
    ok, frame = video.read()
    if not ok:
        break

    # Start empieza el contador
    timer = cv2.getTickCount()

    '''vamos a pasar el fotograma a la funci칩n de seguimiento o actualizaci칩n que nos devolver치 un cuadro delimitado 
    para el objeto detectado ( en caso de encontrarlo)'''
    ok, bbox = tracker.update(frame)

    # calcular los frames por segundo (FPS)
    fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer);

    # dibujar la caja de seguimiento si hemos detectado el objeto
    if ok:
        drawRectangle(frame, bbox)
    else:
        # si no escribir칤amos el texto de fallo en el seguimiento
        drawText(frame, "Tracking failure detected", (80, 140), (0, 0, 255))

    # mostrar la informaci칩n calculada (en 175)
    drawText(frame, tracker_type + " Tracker", (80, 60))
    drawText(frame, "FPS : " + str(int(fps)), (80, 100))

    # escribir el fotograma del v칤deo
    video_out.write(frame)
'''El bucle Recorre cada cuadro en el clip de video y llama a la funci칩n de actualizaci칩n del rastreador y luego anota
los fotogramas y los env칤a al flujo de v칤deo de salida.'''

video.release()
video_out.release()


# ********************************************************
# ***** Deteccion de rostros mediante aprendizaje profundo
# ********************************************************
'''Para detectar los rostros, podemos utilizar OpenCV que nos permitir치 leer en un modelo previamente entrenado y
realizar inferencias usando ese modelo'''
import cv2
import sys

# Establece el 칤ndice para la c치mara si no se introduce otro por par치metro.
s = 0
if len(sys.argv) > 1:
    s = sys.argv[1]

# crea un objeto de captura de v칤deo
source = cv2.VideoCapture(s)

# crea una ventana de salida para enviar todos los resultados a la pantalla
win_name = 'Detecci칩n de c치mara'
cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)

'''OpenCV tiene varias funciones de conveniencia que nos permiten leer y pre-entrenar modelos que fueron entrenados 
usando marcos de trabajo como NetFromCaffe y pytorch que son marcos de aprendizaje profundo que permiten dise침ar y 
entrenar redes neuronales. Adem치s OpenCV tiene una funcionalidad  integrada para usar redes pre-entrenadas para realizar
inferencias ( es decir, no podemos usare OpenCV  para entrenar una red neuronal, pero puede usarlo para realizar 
inferencias en una red entrenada)

la funci칩n cv2.dnn.readNetFromCaffe es una funci칩n dise침ada espec칤ficamente para leer un modelo caffemodel. necesita dos
argumentos:
- El primer argumento aqu칤 es el archivo deploy.prototxt, que contiene la informaci칩n de la arquitectura de la red,
- El segundo archivo es el archivo res10_300x300_ssd_iter_140000_fp16.caffemodel, un archivo mucho m치s grande que 
contiene los pesos del modelo que ha sido entrenado.

en https://github.com/opencv/opencv/tree/4.x/samples/dnn tenemos varios ejemplos de modelos pre entrenados para diversas
utilidades. Hay un archivo L칠ame que contiene una descripci칩n e instrucciones sobre c칩mo usar el script para descargar 
varios modelos. El script hace referencia a un archivo de un modelo con una referencia en el bloque de la parte superior 
al modelo que va a utilizar y la URL para descargar el archivo de pesos, as칤 como otros par치metros relacionados con como
se entren칩 ese modelo como el factor de escala, alto, ancho y rgb.

Cuando llamamos a este m칠todo readNetFromCaffe, regresa para una instancia de la red neuronal, cuyo objeto se usar치 a 
continuaci칩n para realizar inferencias en nuestras im치genes de prueba de la transmisi칩n de video'
'''
net = cv2.dnn.readNetFromCaffe("deploy.prototxt", "res10_300x300_ssd_iter_140000_fp16.caffemodel")

'''Identifica los par치metros del modelo que se asociaron con la forma en que se realiz칩 el modelo entrenado siendo 
importante porque cualquier imagen que pasemos a trav칠s del modelo para realizar la inferencia tambi칠n deben procesarse
de la misma manera que se procesaron las im치genes de entrenamiento.'''
in_width = 300  # se usaron im치genes de 300x300 para entrenar este modelo
in_height = 300
mean = [104, 117, 123]  # lista de valores medios de los canales de color de las im치genes usadas en el entrenamiento
conf_threshold = 0.7  # Umbral de competencia, es un valor que determinar치 la sensibilidad de las detecciones

while cv2.waitKey(1) != 27:  # mientras no pulsemos la tecla con ord 27 (esc)
    has_frame, frame = source.read()  # leemos un fotograma del v칤deo
    if not has_frame:  # lo comprobamos
        break
    frame = cv2.flip(frame, 1)  # giramos horizontalmente el fotograma para mejor interpretaci칩n visual de las se침ales
    # se recupera el tama침o del fotograma
    frame_height = frame.shape[0]
    frame_width = frame.shape[1]

    # Cree un blob 4D a partir de un fotograma.
    '''Estamos haciendo un preprocesamiento en el fotograma, llamando a este m칠todo blobFromImage. lo que 
    realiza es un preprocesamiento en la imagen de entrada y ponerla en el formato adecuado para que luego podamos 
    realizar inferencias en esa imagen. argumentos:
    - fotograma de la imagen
    - factor de escala (1.0) no tiene que ser el mismo siempre
    - ancho y alto del fotograma (300x300)
    - valor medio que se va a restar de todos los fotogramas
    - cambio de flag swapRB (rojo azul), en este caso no es necesario porque caffemodel y OpenCV usan la misma conveci칩n
     para los 3 canales de color
    - Recorte de argumento de entrada, indica que puede recortar su imagen de entrada para que tenga el tama침o correcto 
    o puede cambiar su tama침o, al ponerlo a False, significa que simplemente vamos a cambiar el tama침o de la imagen para
     300x300
     La llamada a la funci칩n devuelve una representaci칩n del blob del fotograma con el pre-procesamiento realizado'''
    blob = cv2.dnn.blobFromImage(frame, 1.0, (in_width, in_height), mean, swapRB=False, crop=False)
    # Corremos el modelo
    net.setInput(blob)  # pasamos el blob a esta funci칩n, establecemos la entrada, prepara para la inferencia
    detections = net.forward()  # Avanza a trav칠s de la red, realiza la inferencia sobre la representaci칩n del fotograma

    for i in range(detections.shape[2]):  # para las detecciones devueltas por la inferencia las recorre
        confidence = detections[0, 0, i, 2]
        # Determina si la competencia de una detecci칩n particular excede el umbral de detecci칩n establecido
        if confidence > conf_threshold:
            '''si lo hace profundiza y consulta en la lista de detecciones las coordenadas del fotograma de esa  
            detecci칩n en particular.'''
            x_left_bottom = int(detections[0, 0, i, 3] * frame_width)
            y_left_bottom = int(detections[0, 0, i, 4] * frame_height)
            x_right_top = int(detections[0, 0, i, 5] * frame_width)
            y_right_top = int(detections[0, 0, i, 6] * frame_height)
            '''Genera un cuadro delimitador ( rect치ngulo) con los puntos de coordenadas obtenidos, as칤 como un texto con
             el % de confiaza de la detecci칩n'''
            cv2.rectangle(frame, (x_left_bottom, y_left_bottom), (x_right_top, y_right_top), (0, 255, 0))
            label = "Confidence: %.4f" % confidence
            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
            # y lo dibuja en el fotograma
            cv2.rectangle(frame, (x_left_bottom, y_left_bottom - label_size[1]),
                                (x_left_bottom + label_size[0], y_left_bottom + base_line),
                                (255, 255, 255), cv2.FILLED)
            cv2.putText(frame, label, (x_left_bottom, y_left_bottom),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))

    '''Una vez que ha terminado de procesar todas las detecciones, llama a getPerfProfile, que devuelve el tiempo 
    necesitado para realizar la inferencia, lo convertimos a milisegundos y lo introduce en la imagen'''
    t, _ = net.getPerfProfile()
    label = 'Inference time: %.2f ms' % (t * 1000.0 / cv2.getTickFrequency())
    cv2.putText(frame, label, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0))
    cv2.imshow(win_name, frame)  # muestra el fotograma relleno en la ventana de salida

source.release()
cv2.destroyWindow(win_name)

# ********************************************************
# ***** Detecci칩n de Objetos mediante aprendizaje profundo
# ********************************************************
# 1.Arquitectura: Multi-Box (SSD) basado en Mobilenet
# 2.Marco: Tensorflow
'''
SSD significa detecci칩n de caja m칰ltiple de un solo disparo. "un solo disparo" se refiere a que vamos a hacer un 칰nico
pase hacia adelante por la red neuronal para realizar inferencias y, sin embargo, detectar m칰ltiples objetos dentro de
una imagen. Al igual que otros tipos de redes, los modelos SSD se pueden entrenar con diferentes estructuras troncales
arquitect칩nicas, lo que esencialmente significa que puede modelar un solo concepto pero usar diferentes columnas
dependiendo de la solicitud.

Entonces, en este caso, estamos usando una arquitectura de red m칩vil, que es un modelo m치s peque침o dise침ado para
dispositivos m칩viles.

# Descargar archivos del repositorio oficial de TensorFlow, con numerosos modelos disponibles
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md

**The cell given below downloads a mobilenet model**
## Download mobilenet model file
The code below will run on Linux / MacOS systems.
Please download the file http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz

Uncompress it and put it in models folder.
'''
import os
import cv2
import numpy as np
import urllib
import matplotlib.pyplot as plt

# **** Crear un archivo de configuraci칩n a partir de un gr치fico congelado
''' NO funciona con tensorflow macosx 2.11 tf_text_graph_ssd.py por el m칩dulo tensorflow.tools.graph_transforms
1. Extrae los archivos
2. Ejecute el archivo tf_text_graph_ssd.py con la entrada como ruta al archivo frozen_graph.pb y la salida como desee.
Se ha incluido un archivo de configuraci칩n de muestra en la carpeta de modelos'''


# frozen_inference_graph.pb, que es el archivo de pesos para el modelo.
modelFile = "models/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb"
# Archivo de configuraci칩n para la red
configFile = "models/ssd_mobilenet_v2_coco_2018_03_29.pbtxt"
# etiquetas de clase para el conjunto de datos que se us칩 para entrenar este modelo
classFile = "coco_class_labels.txt"

if not os.path.isdir('models'):
    os.mkdir("models")

if not os.path.isfile(modelFile):
    os.chdir("models")
    # Download the tensorflow Model
    urllib.request.urlretrieve('http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz', 'ssd_mobilenet_v2_coco_2018_03_29.tar.gz')

    # Se descomprime el fichero

    # Se borra el comprimido
    os.remove('ssd_mobilenet_v2_coco_2018_03_29.tar.gz')

    # volvemos al directorio anterior
    os.chdir("../Training")

'''Hay una gran diferencia entre un detector de objetos de aprendizaje profundo y un objeto de visi칩n artificial 
tradicional ( los revisados hasta ahora) Sol칤amos tener un detector para cada clase, por ejemplo, ten칤amos un detector 
de rostros, un detector de personas y as칤 sucesivamente, todos modelos separados. Pero con los modelos de aprendizaje 
profundo, tenemos una enorme capacidad para aprender. Por lo tanto, un solo modelo puede detectar m칰ltiples objetos en 
una amplia gama de 치ngulos de aspecto y escalas, lo que es la verdadera belleza del aprendizaje profundo'''
# ***** Comprobar las etiquetas de la clase
with open(classFile) as fp:
    labels = fp.read().split("\n")
print(labels)


# *****  leer el modelo de Tensorflow
# Toma como entrada, un archivo de modelo y el archivo de configuraci칩n y nos devolver치 una instancia de la red
net = cv2.dnn.readNetFromTensorflow(modelFile, configFile)


# ***** Detectar Objetos
# Definimos una funci칩n para detectar archivos, Para cada archivo en el directorio
def detect_objects(net, im):  # toma como entrada la instancia de la red y la imagen
    dim = 300

    # Crea un blob a partir de la imagen,
    '''cuando preparamos una imagen para la inferencia, necesitamos realizar cualquier preprocesamiento en esa archivo 
    que se realiz칩 en el conjunto de entrenamiento. Esta funci칩n contiene varios argumentos relacionados con el 
    preprocesamiento requerido.
    - La imagen, 
    - Factor de escala, establecido en uno que indica que el conjunto de entrenamiento no se le realiz칩 ninguna 
      escala especial.
    - tama침o de las im치genes de entrenamiento, (dim=300) por lo que la imagen de prueba, 
      deber치n ser remodelados de acuerdo con este tama침o.
    - valor medio, Si a las im치genes de entrenamiento se les hubiera aplicado un valor medio sustra칤do, entonces esto 
     habr칤a sido otro vector, estas im치genes no requieren ninguna resta de medios, simplemente estamos indicando 0.
    - swapRB por si queremos o no cambiar  loa canales de colores rojo y azul. EN este ejemplo queremos hacer eso, ya 
      que las im치genes de entrenamiento usan una convenci칩n diferente que lo que usa OpenCV.
    - Flag de recorte, que se establece como predeterminada, es decir, las im치genes simplemente cambiar치n de tama침o en 
    lugar de recortarlas a la derecha.
    Esta funci칩n nos devuelve una representaci칩n de blob de esa imagen que ha sido preprocesada, con lo que hay un paso 
    de procesamiento previo, y luego tambi칠n hay un paso de conversi칩n de formato'''
    blob = cv2.dnn.blobFromImage(im, 1.0, size=(dim, dim), mean=(0, 0, 0), swapRB=True, crop=False)

    # pasa el blob a la red neuronal como entrada
    net.setInput(blob)

    # realiza la predicci칩n, se realiza la inferencia en la imagen mediante el m칠todo net.forward()
    objects = net.forward()
    return objects


def display_text(im, text, x, y):  # toma le fotograma, el texto y coordenadas
    '''anotar치 un cuadro delimitador con la etiqueta de clase dibujando un rect치ngulo negro y lo  mete en el fotograma
    con alg칰n texto que indique la etiqueta de clase dentro del negro'''
    # Obtener el tama침o del texto
    textSize = cv2.getTextSize(text, FONTFACE, FONT_SCALE, THICKNESS)
    dim = textSize[0]
    baseline = textSize[1]

    # Usa el tama침o del texto para crear un rect치ngulo negro
    cv2.rectangle(im, (x, y - dim[1] - baseline), (x + dim[0], y + baseline), (0, 0, 0), cv2.FILLED);
    # Display text inside the rectangle
    cv2.putText(im, text, (x, y - 5), FONTFACE, FONT_SCALE, (0, 255, 255), THICKNESS, cv2.LINE_AA)


# **** Mostrar Objetos
# configuraci칩n del texto
FONTFACE = cv2.FONT_HERSHEY_SIMPLEX
FONT_SCALE = 0.7
THICKNESS = 1


# toma el fotograma, una lista de objetos detectados y el umbral de detecci칩n
def display_objects(im, objects, threshold=0.25):
    rows = im.shape[0];
    cols = im.shape[1]

    # Para cada objeto detectado
    for i in range(objects.shape[2]):
        # Encuentra la clase y la confianza
        classId = int(objects[0, 0, i, 1])  # recupera su ID de clase
        score = float(objects[0, 0, i, 2])  # y sus puntuaciones

        # Recuperar las coordenadas originales de las coordenadas normalizadas para el cuadro delimitador
        x = int(objects[0, 0, i, 3] * cols)
        y = int(objects[0, 0, i, 4] * rows)
        w = int(objects[0, 0, i, 5] * cols - x)
        h = int(objects[0, 0, i, 6] * rows - y)

        # Comprueba si la detecci칩n es de buena calidad
        if score > threshold:
            display_text(im, "{}".format(labels[classId]), x, y)  # llama a la funci칩n arriba definida
            cv2.rectangle(im, (x, y), (x + w, y + h), (255, 255, 255), 2)  # introduce en la imagen un rect치ngulo blanco

    # Convertir imagen a RGB, ya que estamos usando Matplotlib para mostrar la imagen
    mp_img = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(30, 10));
    plt.imshow(mp_img);
    plt.show();

# **** Resultados
'''estamos leyendo en una imagen de prueba, y ahora vamos a usar la funci칩n que creamos para detectar los objetos que 
pasan en la instancia de red y la imagen le칤da, retornando la lista de objetos detectados'''
im = cv2.imread('images/street.jpg')
# llamamos a la funci칩n de visualizaci칩n de objetos pasando la imagen de prueba y la matriz de objetos.
objects = detect_objects(net, im)
# este es un algoritmo de detecci칩n de objetos muy robusto que tiene alrededor de 80 clases.
display_objects(im, objects)

im = cv2.imread('images/baseball.jpg')
objects = detect_objects(net, im)
display_objects(im, objects, 0.2)

im = cv2.imread('images/soccer.jpg')
objects = detect_objects(net, im)
display_objects(im, objects)

# *******************************************************************
# ***** Estimacion de la pose humana mediante el aprendizaje profundo
# *******************************************************************
'''
La estimaci칩n de la pose humana puede ser dif칤cil:
 - Los contornos es no siempre son muy visibles
 - la ropa u otra los objetos pueden oscurecer a칰n m치s la imagen.
 - la complejidad a침adida de no solo identificar los puntos clave, sino tambi칠n asociarlos con las personas adecuadas

Usaremos el modelo Open Pose Cafe que se entren칩 en el multiprop칩sito conjunto de datos de imagen, y lo haremos usando
una sola imagen, se침alando antes que la estimaci칩n de la pose humana a menudo se aplica a las transmisiones de video
para varias aplicaciones, como entrenadores inteligentes, por ejemplo.
'''
import cv2
import time
import numpy as np
import matplotlib.pyplot as plt
import os
from IPython import get_ipython

#
# # Cargamos el Modelo  si no est치 en el directorio
# if not os.path.isdir('model'):
#   os.mkdir("model")
#
protoFile = "model/pose_deploy_linevec_faster_4_stages.prototxt"
weightsFile = "model/pose_iter_160000.caffemodel"
#
# # Descargamos el modelo si no se encuentra en el directorio
# if not os.path.isfile(protoFile):
#   # Descargamos el archivo del prototipo
#   get_ipython().system('wget https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/models/pose/mpi/pose_deploy_linevec_faster_4_stages.prototxt -O $protoFile')
#
# if not os.path.isfile(weightsFile):
#   # Descargamso el modelo con el archivo de lso pesos de la red
#   get_ipython().system('wget http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/mpi/pose_iter_160000.caffemodel -O $weightsFile')

# Especificamos el n칰mero de puntos en el modelo y el asociado de pares de ligamiento por sus 칤ndices
'''
cada uno  de estos bloques aqu칤 se refiere a un v칤nculo en la anatom칤a humana:
- 0 -> cabeza.
- 1 -> cuello
- 2 -> hombro derecho 
- 3 -> codo derecho
... y as칤 sucesivamente

Es un mapeo que el modelo usa durante el entrenamiento, y vamos a necesitar este mapeo para procesar la salida de la red
'''
nPoints = 15
POSE_PAIRS = [[0, 1], [1, 2], [2, 3], [3, 4], [1, 5], [5, 6], [6, 7], [1, 14], [14, 8], [8, 9], [9, 10], [14, 11],
              [11, 12], [12, 13]]
# leemos el modelo pasamos el archivo del prototipo y los pesos y nos devolver치 una instancia de la red que usaremos
# en la inferencia
net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)

# leemos la imagen
im = cv2.imread("Tiger_Woods.png")
im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)  # intercambiamos los canales de color rojo y az칰l
# recuperamos el tama침o de la imagen
inWidth = im.shape[1]
inHeight = im.shape[0]

'''cuando preparamos una imagen para la inferencia, necesitamos realizar cualquier preprocesamiento en esa archivo 
    que se realiz칩 en el conjunto de entrenamiento. Esta funci칩n contiene varios argumentos relacionados con el 
    preprocesamiento requerido.
    - La imagen, 
    - Factor de escala, que es el mismo factor de escala que se aplic칩 a las im치genes de entrenamiento. As칤 que 
      necesitamos realizar esa misma transformaci칩n aqu칤 en la imagen de entrada.
    - tama침o de las im치genes de entrenamiento, (netInputSize) por lo que la imagen de prueba, 
      deber치n ser remodelados de acuerdo con este tama침o.
    - valor medio, Si a las im치genes de entrenamiento se les hubiera aplicado un valor medio sustra칤do, entonces esto 
     habr칤a sido otro vector, estas im치genes no requieren ninguna resta de medios, simplemente estamos indicando 0.
    - swapRB por si queremos o no cambiar  loa canales de colores rojo y azul. EN este ejemplo queremos hacer eso, ya 
      que las im치genes de entrenamiento usan una convenci칩n diferente que lo que usa OpenCV.
    - Flag de recorte, que se establece como predeterminada, es decir, las im치genes simplemente cambiar치n de tama침o en 
    lugar de recortarlas a la derecha.
    Esta funci칩n nos devuelve una representaci칩n de blob de esa imagen que ha sido preprocesada, con lo que hay un paso 
    de procesamiento previo, y luego tambi칠n hay un paso de conversi칩n de formato
'''
netInputSize = (368, 368)
# Convertimos la imagen a blob
inpBlob = cv2.dnn.blobFromImage(im, 1.0 / 255, netInputSize, (0, 0, 0), swapRB=True, crop=False)
net.setInput(inpBlob)

# realiza la predicci칩n, se realiza la inferencia en la imagen mediante el m칠todo net.forward(), devuelve es la salida
# de la red, que consta de mapas de confianza y afinidad.
output = net.forward()

# Mostrar mapas de probabilidad
'''
solo usaremos los mapas de confianza para realizar la clave detecci칩n de puntos en esta demostraci칩n. para cada punto, 
vamos a recibir un mapa de probabilidad '''
plt.figure(figsize=(20, 10))
plt.title('Probability Maps of Keypoints')
for i in range(nPoints):
    probMap = output[0, i, :, :]  # recibimos ese mapa de probabilidad
    '''y luego simplemente vamos a trazar cada uno de estos mapas de probabilidad y se podr치 observar que est치n 
    codificados por colores, sus mapas de calor que indican la probabilidad, de la ubicaci칩n del punto clave detectado.
    El rojo es una probabilidad muy alta. en cada uno de estos mapas de probabilidad, la ubicaci칩n probable para un 
    punto clave (punto cero, cabeza,  uno cuello  y as칤 sucesivamente.
    '''
    displayMap = cv2.resize(probMap, (inWidth, inHeight), cv2.INTER_LINEAR)
    plt.subplot(3, 5, i + 1);
    plt.axis('off');
    plt.imshow(displayMap, cmap='jet')

'''Podemos usar estos mapas de probabilidad para superponer esos puntos clave en la imagen original. Y para hacer eso,
vamos a tener que escalarlos en la misma escala que la imagen de entrada. Estamos usando la forma de salida de la red, 
es decir, la forma de los mapas de probabilidad y tambi칠n la forma de entrada de la imagen de prueba para calcular a 
escala los factores X e Y que terminaremos usando a continuaci칩n para determinar la ubicaci칩n de los puntos clave en 
la imagen de prueba real.
Antes, vamos a necesitar determinar la ubicaci칩n de los puntos clave en el mapa de probabilidad
'''

# ***** Extraemos los puntos

# X and Y Scale
scaleX = float(inWidth) / output.shape[3]
scaleY = float(inHeight) / output.shape[2]

# Lista vac칤a para almacenar los puntos clave detectados
points = []

# Umbral de confianza
threshold = 0.1
# Recorre todos los puntos clave, y para cada punto clave, vamos a recuperar el mapa de probabilidad de la matriz de
# salida de la red.
for i in range(nPoints):
    # Obtener mapa de probabilidad
    probMap = output[0, i, :, :]

    # Encuentra los m치ximos globales del probMap.
    '''llamamos a la funci칩n de OpenCV cv2.minMaxLoc pas치ndole el mapa de probabilidad. Y esto va devolver La ubicaci칩n
    del punto asociado con la m치xima probabilidad.
    * En point se encuentran las coordenadas del punto
    '''
    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)

    '''Una vez que tengamos esa ubicaci칩n en las coordenadas del mapa de probabilidad, la multiplicaremos por los 
    factores de escala X e Y que calculamos arriba para obtener la ubicaci칩n del punto clave en la imagen de prueba 
    original.'''
    # Escale el punto para que encaje en la imagen original
    x = scaleX * point[0]
    y = scaleY * point[1]

    if prob > threshold:  # Si la probabilidad devuelta es mayor que el umbral
        # Tomamos ese punto, agreg치ndolo a la lista.
        points.append((int(x), int(y)))
    else:
        points.append(None)

# Y ahora estamos listos para renderizar esos puntos en la imagen de prueba.

# **** Puntos de visualizaci칩n y esqueleto
# Estamos haciendo una copia de la imagen de entrada, en uno la llamamos punto y en otro esqueleto
imPoints = im.copy()
imSkeleton = im.copy()

# **** Dibujamos puntos
'''vamos a recorrer todos los puntos que fueron los que acabamos de crear en los bucles anteriores. Y esas son las 
coordenadas de los puntos clave en el cuadro de coordenadas de la imagen de prueba.
'''
for i, p in enumerate(points):
    # vamos a usar el c칤rculo y el texto para dibujar y etiquetar esos puntos en la imagen de los puntos finales (izq)
    cv2.circle(imPoints, p, 8, (255, 255, 0), thickness=-1, lineType=cv2.FILLED)
    cv2.putText(imPoints, "{}".format(i), p, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, lineType=cv2.LINE_AA)

# dibujar esqueleto
'''
vamos a renderizar la vista de esqueleto (derecha del resultado). Con este ciclo for, estamos recorriendo todos los 
pares de publicaciones, que definimos antes, y luego estamos recuperando esos pares y vamos a configurar esas dos 
partes A y parte B aqu칤 y luego utilizarlas como 칤ndices.
'''
for pair in POSE_PAIRS:
    partA = pair[0]
    partB = pair[1]
    '''
    Ingresamos la lista de puntos que creamos anteriormente, que contiene la lista de ubicaciones de puntos clave en la 
    imagen de prueba Y ahora simplemente vamos a usar las funciones de c칤rculo y l칤nea CV abiertas para dibujar una 
    l칤nea desde un punto hasta el siguiente codificado por colores, adem치s de dibujar un c칤rculo en el primer punto 
    clave en ese enlace.'''
    if points[partA] and points[partB]:
        cv2.line(imSkeleton, points[partA], points[partB], (255, 255, 0), 2)
        cv2.circle(imSkeleton, points[partA], 8, (255, 0, 0), thickness=-1, lineType=cv2.FILLED)

plt.figure(figsize=(20, 10))
plt.subplot(121);
plt.axis('off');
plt.imshow(imPoints);  # Usamos plt.imshow para mostrar ambas im치genes
# plt.title('Displaying Points')
plt.subplot(122);
plt.axis('off');
plt.imshow(imSkeleton);
# plt.title('Displaying Skeleton')
plt.show()


